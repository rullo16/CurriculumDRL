{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import SAC_ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrullofederico16\u001b[0m (\u001b[33mfede-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\wandb\\run-20250329_124720-kdxzsk9r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fede-/SAC_Distillation/runs/kdxzsk9r' target=\"_blank\">glamorous-dew-260</a></strong> to <a href='https://wandb.ai/fede-/SAC_Distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fede-/SAC_Distillation' target=\"_blank\">https://wandb.ai/fede-/SAC_Distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fede-/SAC_Distillation/runs/kdxzsk9r' target=\"_blank\">https://wandb.ai/fede-/SAC_Distillation/runs/kdxzsk9r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"SAC_Distillation\", entity=\"fede-\")\n",
    "wandb.config.update(params['sac_distilled'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.config.update({\"device\": device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"Env/DroneFlightv1\", seed=1, no_graphics_monitor=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=9', 'Drone?team=0?agent_id=9']\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = SAC_ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape,env.action_space(agents[0]).shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledSAC(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape,len(agents), params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in range(1, params['sac_distilled'].seed_episodes):\n",
    "#     obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "#     while not all(done) or t < params['sac_distilled'].n_steps_random_exploration:\n",
    "#         actions = {}\n",
    "#         log_probs = {}\n",
    "#         values = {}\n",
    "#         agents = relocate_agents(env)\n",
    "#         for agent in agents:\n",
    "#             actions[agent] = env.action_space(agent).sample()\n",
    "#             if agent not in obs.keys():\n",
    "#                 continue\n",
    "#             obs1, obs2 = get_agent_obs(obs, agent)\n",
    "#             v1,v2 = brain.get_values(obs1,obs2, actions[agent],t)\n",
    "#             values[agent] = torch.min(v1,v2)\n",
    "\n",
    "#         actions = {agent: act for agent, act in actions.items() if agent in set(env.agents)}\n",
    "#         next_obs, reward, done, _ = env.step(actions)\n",
    "\n",
    "#         for agent in agents:\n",
    "#             if agent not in next_obs.keys():\n",
    "#                 continue\n",
    "#             next_obs1, next_obs2 = get_agent_obs(next_obs, agent)\n",
    "#             Buffer.store(obs1, obs2, actions[agent], reward[agent], next_obs1, next_obs2, done[agent])\n",
    "#         obs = next_obs\n",
    "#         done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "#         t += 1\n",
    "#     print(f'Finished episode {s}')\n",
    "\n",
    "# # Buffer.compute_advantages()\n",
    "# print(\"Finished Rnd Exploration\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer.save('Collected_Experience/init_experience.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer.load('Collected_Experience/init_experience.npz')\n",
    "# Buffer.size=len(Buffer.camera_obs)\n",
    "# Buffer.ptr=Buffer.size\n",
    "# print(len(Buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain.fine_tune_teacher(Buffer, epochs=params['sac_distilled'].train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain.teacher.save(\"SavedModels/Teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain.train(Buffer,step = params['sac_distilled'].seed_episodes*params['sac_distilled'].n_steps_random_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer._clear_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain.save(\"SavedModels/SAC_distilled_checkpoint.pth\")\n",
    "# print(\"Checkpoint saved successfully.\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brain.load(\"SavedModels/SAC_distilled_checkpoint.pth\")\n",
    "# print(\"Checkpoint loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UE(file_name=\"Env/DroneFlightv1\", seed=1, no_graphics_monitor=True)\n",
    "# env = UPZBE(env)\n",
    "# env.reset()\n",
    "# agents = relocate_agents(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m actor_loss, critic_loss, entropy_loss, distill_loss \u001b[38;5;241m=\u001b[39m \u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m mean_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([values[agent]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents])\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m critic_loss \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e6\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\SAC_Distillation\\DistilledSACAgent.py:246\u001b[0m, in \u001b[0;36mDistilledSAC.train\u001b[1;34m(self, trajectories, step)\u001b[0m\n\u001b[0;32m    244\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(replay_buffer, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m camera_obs, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m--> 246\u001b[0m     camera_obs \u001b[38;5;241m=\u001b[39m \u001b[43mcamera_obs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     student_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconvolution_pipeline(camera_obs)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps=0\n",
    "obs = env.reset()\n",
    "mean_reward = -np.inf  # or 0\n",
    "while steps < params['sac_distilled'].max_steps:\n",
    "    # obs = env.reset()\n",
    "    done = [False for _ in env.agents]\n",
    "    episode_rewards, t = [], 0\n",
    "    while not all(done) or t < params['sac_distilled'].n_steps:\n",
    "        actions, values = {}, {}\n",
    "        agents = relocate_agents(env)\n",
    "\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "\n",
    "            camera_obs, pos_obs = get_agent_obs(obs, agent)\n",
    "            action, _ = brain.get_action(camera_obs, pos_obs, train=True)\n",
    "\n",
    "            v1,v2 = brain.get_values(camera_obs, pos_obs, action, steps+t)\n",
    "            value = torch.min(v1,v2)\n",
    "\n",
    "            actions[agent] = action.cpu().detach().numpy().flatten().tolist()\n",
    "            values[agent] = value\n",
    "        next_obs, reward, done, _ = env.step(actions)\n",
    "\n",
    "        for agent in agents:\n",
    "            if agent not in next_obs.keys():\n",
    "                continue\n",
    "            camera_obs, pos_obs = get_agent_obs(obs, agent)\n",
    "            next_camera_obs, next_pos_obs = get_agent_obs(next_obs, agent)\n",
    "            reward[agent] = reward[agent] * 10\n",
    "            Buffer.store(camera_obs, pos_obs, actions[agent], reward[agent], next_camera_obs, next_pos_obs, done[agent])\n",
    "\n",
    "        obs = next_obs\n",
    "        t += 1\n",
    "        episode_rewards.append(list(reward.values()))\n",
    "\n",
    "    # Compute reward properly\n",
    "    episode_flat_rewards = [r for r_list in episode_rewards for r in r_list]\n",
    "    current_reward = np.mean(episode_flat_rewards)\n",
    "\n",
    "    if current_reward > mean_reward:\n",
    "        brain.save(\"SavedModels/SAC_distilled_trained.pth\")\n",
    "        print(\"Checkpoint saved successfully.\")\n",
    "        mean_reward = current_reward\n",
    "\n",
    "    steps += t\n",
    "\n",
    "    # Training step\n",
    "    actor_loss, critic_loss, entropy_loss, distill_loss = brain.train(Buffer, steps)\n",
    "    mean_val = np.mean([values[agent].mean().item() for agent in env.agents])\n",
    "    if critic_loss > 1e6:\n",
    "        brain.load(\"SavedModels/SAC_distilled_trained.pth\")\n",
    "        print(\"Checkpoint loaded successfully.\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"Mean Reward\": current_reward,\n",
    "        \"Steps\": steps,\n",
    "        \"Actor Loss\": actor_loss,\n",
    "        \"Critic Loss\": critic_loss,\n",
    "        \"Entropy Loss\": entropy_loss,\n",
    "        \"Distill Loss\": distill_loss,\n",
    "        \"value\": mean_val\n",
    "    })\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
