{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import SAC_ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return sorted(list(env.agents))\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent, *, cam_key=1, vec_keys=[0,2]):\n",
    "    if agent not in obs:\n",
    "        raise KeyError(f\"Agent {agent!r} not found in obs (keys: {list(obs.keys())[:8]}...)\")\n",
    "\n",
    "    data = obs[agent]\n",
    "    if isinstance(data, dict) and \"observation\" in data:\n",
    "        data = data[\"observation\"]\n",
    "\n",
    "    # Case A: explicit keys\n",
    "    if isinstance(data, dict) and (\"camera_obs\" in data and \"vector_obs\" in data):\n",
    "        cam = np.asarray(data[\"camera_obs\"])\n",
    "        vec = np.asarray(data[\"vector_obs\"])\n",
    "        # ensure vec is 1D\n",
    "        if vec.ndim > 1:\n",
    "            vec = vec.reshape(-1)\n",
    "    else:\n",
    "        # Case B: indexed container\n",
    "        cam = np.asarray(data[cam_key])\n",
    "        v0 = np.asarray(data[vec_keys[0]]).reshape(-1)\n",
    "        v1 = np.asarray(data[vec_keys[1]]).reshape(-1)\n",
    "        vec = np.concatenate([v0, v1], axis=0)\n",
    "\n",
    "    # ---- Camera post-processing: to CHW float32 in [0,1] ----\n",
    "    if cam.ndim != 3:\n",
    "        raise AssertionError(f\"Camera observation must be 3D (HWC or CHW), got shape {cam.shape}\")\n",
    "\n",
    "    # If HWC (channel last), move to CHW\n",
    "    if cam.shape[-1] in (1, 3, 4):\n",
    "        cam = np.transpose(cam, (2, 0, 1))\n",
    "\n",
    "    cam = cam.astype(np.float32, copy=False)\n",
    "    if cam.max() > 1.5:  # likely uint8 [0..255]\n",
    "        cam = cam / 255.0\n",
    "\n",
    "    vec = vec.astype(np.float32, copy=False)\n",
    "\n",
    "    return cam, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"Env/Level1/DroneFlightv1\", seed=1)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = relocate_agents(env)\n",
    "N_AGENTS = len(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "cam_shape = env.observation_space(agents[0])[1].shape\n",
    "vec_dim = env.observation_space(agents[0])[0].shape[0] + env.observation_space(agents[0])[2].shape[0]\n",
    "vec_shape = (vec_dim,)\n",
    "action_shape = env.action_space(agents[0]).shape\n",
    "print(\"Agents:\", N_AGENTS, \"| cam_shape:\", cam_shape, \"| vec_dim:\", vec_shape[0], \"| act:\", action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = SAC_ExperienceBuffer(cam_shape, vec_shape,action_shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DistilledSAC(cam_shape, vec_shape, action_shape,len(agents), params['sac_distilled'])\n",
    "\n",
    "feat_path = \"SavedModels/feature_extractor_contrastive_init.pth\"\n",
    "if os.path.isfile(feat_path):\n",
    "    state = torch.load(feat_path, map_location=device)\n",
    "    agent.model.convolution_pipeline.load_state_dict(state, strict=False)\n",
    "else:\n",
    "    print(f\"⚠️  Init features not found: {feat_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = params['sac_distilled']\n",
    "RAND_STEPS = cfg.get(\"n_steps_random_exploration\", 100_000)\n",
    "SEED_EPISODES = cfg.get(\"seed_episodes\", 1)\n",
    "blank_cam = np.zeros(cam_shape, dtype=np.float32)\n",
    "blank_vec = np.zeros(vec_shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "run_name = f\"sac_distill_{dt.datetime.now():%Y%m%d_%H%M%S}_level_1\"\n",
    "wandb.init(\n",
    "            project=os.getenv(\"WANDB_PROJECT\", \"SAC_Distillation\"),\n",
    "            entity =os.getenv(\"WANDB_ENTITY\",  \"fede-\"),\n",
    "            name=run_name,\n",
    "            config = {**params[\"sac_distilled\"], \"device\": str(device)},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config / counters ---\n",
    "total_updates = 0\n",
    "train_every   = 4096\n",
    "log_every     = 4096\n",
    "print_every   = 10000\n",
    "max_steps     = cfg.get(\"max_steps\", 5_000_000)\n",
    "\n",
    "ema_reward      = 0.0\n",
    "last_ema_reward = -np.inf\n",
    "ema_alpha       = cfg.get(\"ema_alpha\", 0.01)\n",
    "\n",
    "# --- Reset & stable agent order (keep constant over run) ---\n",
    "obs   = env.reset()\n",
    "AGENTS = relocate_agents(env)            # must preserve order\n",
    "N_AGENTS = len(AGENTS)\n",
    "\n",
    "# Expect get_agent_obs to return CHW float32 in [0,1]; vec 1D float32\n",
    "blank_cam = np.zeros((cam_shape), dtype=np.float32)\n",
    "blank_vec = np.zeros((vec_shape), dtype=np.float32)\n",
    "\n",
    "# --- Preallocate step buffers (reuse each step) ---\n",
    "cam_now   = np.empty((N_AGENTS, *cam_shape), dtype=np.float32)\n",
    "vect_now  = np.empty((N_AGENTS, *vec_shape), dtype=np.float32)\n",
    "cam_next  = np.empty_like(cam_now)\n",
    "vect_next = np.empty_like(vect_now)\n",
    "rew_now   = np.empty((N_AGENTS, 1), dtype=np.float32)\n",
    "done_now  = np.zeros((N_AGENTS, 1), dtype=np.float32)   # continuous env: always zeros\n",
    "\n",
    "steps = 0\n",
    "goal_reached = 0.0\n",
    "crashed = 0.0\n",
    "\n",
    "\n",
    "# Mapping from canonical AGENTS -> index\n",
    "agent_to_idx = {aid: i for i, aid in enumerate(AGENTS)}\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while steps < max_steps:\n",
    "    # If no live agents (obs empty), reset and continue\n",
    "    if not obs or (isinstance(obs, dict) and len(obs) == 0):\n",
    "        obs = env.reset()\n",
    "        continue\n",
    "\n",
    "    # Live agents this step (use obs keys)\n",
    "    live = relocate_agents(env)\n",
    "\n",
    "    # --- PACK CURRENT OBS INTO FIXED SLOTS ---\n",
    "    # Start with blanks\n",
    "    cam_now[:] = blank_cam\n",
    "    vect_now[:]  = blank_vec\n",
    "    for aid in live:\n",
    "        i = agent_to_idx.get(aid, None)\n",
    "        if i is None:\n",
    "            # New agent id we haven't seen before -> extend canonical arrays (rare)\n",
    "            # Simple choice: skip it (or rebuild AGENTS/arrays if you prefer)\n",
    "            continue\n",
    "        cam, vec = get_agent_obs(obs, aid)  # cam: (C,H,W) float[0,1], vec: (D,)\n",
    "        cam_now[i]  = cam\n",
    "        vect_now[i] = vec\n",
    "\n",
    "    cam_t = torch.from_numpy(cam_now).to(device)\n",
    "    vec_t = torch.from_numpy(vect_now).to(device)\n",
    "    if torch.isnan(cam_t).any() or torch.isnan(vec_t).any():\n",
    "        cam_t = torch.nan_to_num(cam_t); vec_t = torch.nan_to_num(vec_t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        act_t, _ = agent.get_action(cam_t, vec_t, train=True)  # (N_AGENTS, act_dim)\n",
    "    act_np = act_t.detach().cpu().numpy()\n",
    "    np.clip(act_np, -1.0, 1.0, out=act_np)\n",
    "\n",
    "    # --- BUILD ACTIONS ONLY FOR LIVE AGENTS ---\n",
    "    actions = {aid: act_np[agent_to_idx[aid]] for aid in live}\n",
    "\n",
    "    # If somehow live is empty, reset and continue\n",
    "    if len(actions) == 0:\n",
    "        obs = env.reset()\n",
    "        continue\n",
    "\n",
    "    # --- STEP ENV ---\n",
    "    next_obs, rew_dict, done_dict, infos = env.step(actions)\n",
    "    steps += 1\n",
    "\n",
    "    # --- PACK NEXT OBS & REWARDS INTO FIXED SLOTS ---\n",
    "    cam_next[:]  = blank_cam\n",
    "    vect_next[:] = blank_vec\n",
    "    rew_now[:]   = 0.0\n",
    "    # done_now stays 0 in continuous env\n",
    "\n",
    "    # Note: next_obs may also have fewer agents; fill from those keys\n",
    "    next_live = list(next_obs.keys()) if isinstance(next_obs, dict) else []\n",
    "    for aid in next_live:\n",
    "        i = agent_to_idx.get(aid, None)\n",
    "        if i is None:\n",
    "            continue\n",
    "        cam_n, vec_n = get_agent_obs(next_obs, aid)\n",
    "        cam_next[i]  = cam_n\n",
    "        vect_next[i] = vec_n\n",
    "\n",
    "    # Rewards (use the agents we acted for this step)\n",
    "    for aid in live:\n",
    "        i = agent_to_idx.get(aid, None)\n",
    "        if i is None:\n",
    "            continue\n",
    "        r = float(rew_dict.get(aid, 0.0)) + float(infos.get(aid, {}).get(\"reward\", 0.0))\n",
    "        rew_now[i, 0] = r\n",
    "        # event counters (optional thresholds)\n",
    "        if r > 70:   goal_reached += 1\n",
    "        if r < -99:  crashed += 1\n",
    "\n",
    "    # Store transition\n",
    "    priority = 3.0 if (rew_now.max() >= 70.0) else 1.0\n",
    "    replay_buffer.store_joint(\n",
    "        cam_now, vect_now, act_np,\n",
    "        rew_now,\n",
    "        cam_next, vect_next,\n",
    "        done_now,                  # all zeros; continuous env\n",
    "        priority=priority\n",
    "    )\n",
    "\n",
    "    # --- TRAIN / LOG (unchanged, with safe conversions) ---\n",
    "    mean_r     = float(np.mean(rew_now))\n",
    "    ema_reward = ema_reward * (1.0 - ema_alpha) + mean_r * ema_alpha\n",
    "\n",
    "    curr_size = replay_buffer.size() if callable(getattr(replay_buffer, \"size\", None)) else replay_buffer.size\n",
    "    if curr_size < (agent.batch_size * max(1, getattr(agent, \"num_agents\", N_AGENTS))):\n",
    "        if steps % print_every == 0:\n",
    "            print(f\"Steps: {steps} | EMA Reward: {ema_reward:.3f} | Buffer: {curr_size}\", end=\"\\r\")\n",
    "        obs = next_obs\n",
    "        continue\n",
    "\n",
    "    if steps % train_every == 0:\n",
    "        a_loss, c_loss, intrinsic_rew, rnd_loss = agent.train(replay_buffer, step=steps)\n",
    "        total_updates += 1\n",
    "        c_loss_val = float(c_loss.detach().cpu()) if isinstance(c_loss, torch.Tensor) else float(c_loss)\n",
    "        if c_loss_val > 1e6:\n",
    "            try:\n",
    "                agent.load(\"SavedModels/SAC_distilled_trained_level_1.pth\")\n",
    "                print(\"\\n[Guard] Critic loss exploded; reloaded last checkpoint.\", end=\"\\r\")\n",
    "            except Exception:\n",
    "                print(\"\\n[Guard] Critic loss exploded; no checkpoint to load.\", end=\"\\r\")\n",
    "\n",
    "        if ema_reward > last_ema_reward:\n",
    "            last_ema_reward = ema_reward\n",
    "            try:\n",
    "                agent.save(\"SavedModels/SAC_distilled_trained_level_1.pth\")\n",
    "                print(f\"\\n[Checkpoint] New best EMA reward: {last_ema_reward:.2f}\", end=\"\\r\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if steps % log_every == 0:\n",
    "            print(f\"\\nStep: {steps} | Goals: {goal_reached:.0f} | Crashes: {crashed:.0f}\")\n",
    "            print(f\"Losses: Actor={float(a_loss):.4f} | Critic={c_loss_val:.4f} | RND={float(rnd_loss):.4f} | Intr={float(intrinsic_rew):.4f}\",end=\"\\r\")\n",
    "            print(f\"Mean R: {mean_r:.4f} | EMA R: {ema_reward:.4f}\", end=\"\\r\")\n",
    "            if 'wandb' in globals():\n",
    "                wandb.log({\n",
    "                    \"EMA Reward\": float(ema_reward),\n",
    "                    \"Mean Reward\": float(mean_r),\n",
    "                    \"Actor Loss\": float(a_loss),\n",
    "                    \"Critic Loss\": c_loss_val,\n",
    "                    \"Intrinsic Reward\": float(intrinsic_rew),\n",
    "                    \"RND Loss\": float(rnd_loss),\n",
    "                    \"Steps\": int(steps),\n",
    "                    \"Goal Events\": float(goal_reached),\n",
    "                    \"Crash Events\": float(crashed),\n",
    "                    \"Updates\": int(total_updates),\n",
    "                }, step=int(steps))\n",
    "            goal_reached = 0.0\n",
    "            crashed = 0.0\n",
    "\n",
    "    # Keep streaming (continuous env)\n",
    "    obs = next_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "try:\n",
    "    env.close()\n",
    "except Exception:\n",
    "    pass\n",
    "agent.load(path=\"SavedModels/SAC_distilled_trained_level_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"Env/FinalLevel/DroneFlightv1\", seed=1)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = list(set(env.agents))\n",
    "print(agents)\n",
    "print(env.action_spaces)\n",
    "print(env._env.behavior_specs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del replay_buffer\n",
    "replay_buffer = SAC_ExperienceBuffer(cam_shape, vec_shape, action_shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_steps = RAND_STEPS * SEED_EPISODES   # e.g. 5 120 000\n",
    "\n",
    "obs_dict = env.reset()                   # one reset BEFORE the loop\n",
    "\n",
    "for step in range(tot_steps):\n",
    "    if not obs_dict:\n",
    "        obs_dict = env.reset()\n",
    "        continue\n",
    "    agents = relocate_agents(env)  # get the current agents in the environment\n",
    "    # --- draw a random joint action ---------------------------------\n",
    "    act_dict = {a: env.action_space(a).sample() for a in agents}\n",
    "\n",
    "    cam_now  = np.empty((N_AGENTS, *cam_shape),   dtype=np.float32)\n",
    "    vect_now = np.empty((N_AGENTS, *vec_shape),   dtype=np.float32)\n",
    "    act_now  = np.empty((N_AGENTS, *action_shape),dtype=np.float32)\n",
    "\n",
    "    for i, a in enumerate(agents):\n",
    "        cam, vec = get_agent_obs(obs_dict, a) if a in obs_dict else (blank_cam, blank_vec)\n",
    "        cam_now[i], vect_now[i], act_now[i] = cam, vec, act_dict[a]\n",
    "\n",
    "    # --- take one step ----------------------------------------------\n",
    "    next_obs, rew_dict, done_dict, _ = env.step(act_dict)\n",
    "\n",
    "    # --- pack the next‐state tensors --------------------------------\n",
    "    cam_next  = np.empty_like(cam_now)\n",
    "    vect_next = np.empty_like(vect_now)\n",
    "    rew_now   = np.zeros((N_AGENTS, 1), dtype=np.float32)\n",
    "    done_now  = np.zeros((N_AGENTS, 1), dtype=np.float32)\n",
    "\n",
    "    for i, a in enumerate(agents):\n",
    "        cam_n, vec_n = get_agent_obs(next_obs, a) if a in next_obs else (blank_cam, blank_vec)\n",
    "        cam_next[i], vect_next[i] = cam_n, vec_n\n",
    "        rew_now[i, 0]  = rew_dict.get(a, 0.0)\n",
    "        done_now[i, 0] = float(done_dict.get(a, False))\n",
    "\n",
    "    replay_buffer.store_joint(cam_now, vect_now, act_now,\n",
    "                              rew_now, cam_next, vect_next, done_now)\n",
    "\n",
    "    obs_dict = next_obs\n",
    "\n",
    "    # if the whole team is done, start a new episode\n",
    "    if all(done_dict.values()):\n",
    "        obs_dict = env.reset()\n",
    "\n",
    "print(\"Finished collecting random steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "run_name = f\"sac_distill_{dt.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "wandb.init(\n",
    "            project=os.getenv(\"WANDB_PROJECT\", \"SAC_Distillation\"),\n",
    "            entity =os.getenv(\"WANDB_ENTITY\",  \"fede-\"),\n",
    "            name=run_name,\n",
    "            config = {**params[\"sac_distilled\"], \"device\": str(device)},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_updates = 0\n",
    "train_every = 4_096\n",
    "seed_step_offline_distil = 1\n",
    "log_every = 1000\n",
    "print_every = 10_000\n",
    "max_steps = cfg.get(\"max_steps\", 5_000_000)\n",
    "goal_reached = 0.0\n",
    "crashed = 0.0\n",
    "\n",
    "\n",
    "ema_reward = 0.0\n",
    "last_ema_reward = -np.inf\n",
    "ema_alpha = cfg.get(\"ema_alpha\", 0.01)\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed_step_offline_distil > 0:\n",
    "    agent.offline_distill(replay_buffer,batch_size=256)\n",
    "\n",
    "\n",
    "while steps < max_steps:\n",
    "    if not obs:\n",
    "        obs = env.reset()\n",
    "        continue\n",
    "    agents = relocate_agents(env)\n",
    "    cam_now = np.zeros((N_AGENTS, *cam_shape), dtype=np.uint8)\n",
    "    vect_now = np.zeros((N_AGENTS, *vec_shape), dtype=np.float32)\n",
    "\n",
    "    for i, aid in enumerate(agents):\n",
    "        if aid in obs:\n",
    "            cam, vec = get_agent_obs(obs, aid)\n",
    "        else:\n",
    "            cam, vec = blank_cam, blank_vec\n",
    "        cam_now[i]  = cam\n",
    "        vect_now[i] = vec\n",
    "\n",
    "    cam_t = torch.from_numpy(np.stack(cam_now)).float().to(device)\n",
    "    vec_t = torch.from_numpy(np.stack(vect_now)).float().to(device)\n",
    "\n",
    "    if cam_t.isnan().any() or vec_t.isnan().any():\n",
    "        cam_t = torch.nan_to_num(cam_t)\n",
    "        vec_t = torch.nan_to_num(vec_t)\n",
    "    with torch.no_grad():\n",
    "        step_fraction = steps / max_steps\n",
    "        act_t = agent.get_action(cam_t, vec_t, train=False)\n",
    "    \n",
    "\n",
    "    # act_np = torch.round(act_t).clamp(-1,1).cpu().numpy()\n",
    "    act_np = act_t.cpu().numpy()\n",
    "    actions = {aid: action for aid, action in zip(agents, act_np)}\n",
    "    # print(actions)\n",
    "    \n",
    "\n",
    "    next_obs, rew_dict, done_dict, infos = env.step(actions)\n",
    "    steps += 1\n",
    "\n",
    "    cam_next = np.zeros_like(cam_now)\n",
    "    vect_next = np.zeros_like(vect_now)\n",
    "    rew_now = np.zeros((N_AGENTS, 1), dtype=np.float32)\n",
    "    done_now = np.zeros((N_AGENTS, 1), dtype=np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    for i, aid in enumerate(agents):\n",
    "        if aid in next_obs:\n",
    "            cam_n, vec_n = get_agent_obs(next_obs, aid)\n",
    "        else:\n",
    "            cam_n, vec_n = blank_cam, blank_vec\n",
    "        cam_next[i] = cam_n\n",
    "        vect_next[i] = vec_n\n",
    "\n",
    "        r = rew_dict.get(aid, 0.0) + infos.get(aid, {}).get('reward', 0.0)\n",
    "        rew_now[i, 0] = r\n",
    "        done_now[i, 0] = done_dict.get(aid, False)\n",
    "\n",
    "        goal_reached += 1 if r > 19 else 0\n",
    "        crashed += 1 if r < -9 else 0\n",
    "\n",
    "    replay_buffer.store_joint(\n",
    "        cam_now, vect_now, act_np,\n",
    "        rew_now,\n",
    "        cam_next, vect_next,\n",
    "        done_now\n",
    "    )\n",
    "\n",
    "    mean_r = np.mean(rew_now).item()\n",
    "    ema_reward = ema_reward * (1- ema_alpha) + mean_r * ema_alpha\n",
    "    \n",
    "    if replay_buffer.size < agent.batch_size * agent.num_agents:\n",
    "        continue\n",
    "\n",
    "    if steps % train_every == 0:\n",
    "        a_loss, c_loss, intrinsic_rew, rnd_loss = agent.train(replay_buffer, step=steps)\n",
    "        total_updates += 1\n",
    "\n",
    "        if c_loss > 1e6:\n",
    "            agent.load(\"SavedModels/SAC_distilled_trained_fix.pth\")\n",
    "            print(\"Critic loss exploded, reloading model\")\n",
    "        \n",
    "        if ema_reward > last_ema_reward:\n",
    "            last_ema_reward = ema_reward\n",
    "            agent.save(\"SavedModels/SAC_distilled_trained_fix.pth\")\n",
    "            print(f\"New best EMA reward: {last_ema_reward:.2f}\")\n",
    "\n",
    "        if steps % log_every==0:\n",
    "            wandb.log({\n",
    "                \"EMA Reward\": ema_reward,\n",
    "                \"Mean Reward\": mean_r,\n",
    "                \"Actor Loss\": a_loss,\n",
    "                \"Critic Loss\": c_loss,\n",
    "                \"Steps\": steps,\n",
    "                \"Goal Reached\": goal_reached,\n",
    "                \"Crashes\": crashed,\n",
    "                \"Intrinsic Reward\": intrinsic_rew,\n",
    "                \"RND Loss\": rnd_loss,\n",
    "            }, step=steps)\n",
    "            \n",
    "            goal_reached = 0.0\n",
    "            crashed = 0.0\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
