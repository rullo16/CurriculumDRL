{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrullofederico16\u001b[0m (\u001b[33mfede-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\wandb\\run-20250224_224049-key3onsj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fede-/SAC_Distillation/runs/key3onsj' target=\"_blank\">honest-bird-68</a></strong> to <a href='https://wandb.ai/fede-/SAC_Distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fede-/SAC_Distillation' target=\"_blank\">https://wandb.ai/fede-/SAC_Distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fede-/SAC_Distillation/runs/key3onsj' target=\"_blank\">https://wandb.ai/fede-/SAC_Distillation/runs/key3onsj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"SAC_Distillation\", entity=\"fede-\")\n",
    "wandb.config.update(params['sac_distilled'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.config.update({\"device\": device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=9']\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  ...\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]]]\n",
      "Possible actions: [-1 -1  0 -1  0 -1]\n",
      "(6,)\n",
      "(1, 84, 84)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[agents[0]][1])\n",
    "possible_actions = env.action_space(agents[0]).sample()\n",
    "print(f\"Possible actions: {possible_actions}\")\n",
    "print(env.action_space(agents[0]).shape)\n",
    "print(env.observation_space(agents[0])[1].shape)\n",
    "print(env.observation_space(agents[0])[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape,env.action_space(agents[0]).shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledSAC(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Rnd Exploration\n"
     ]
    }
   ],
   "source": [
    "for s in range(1, 1):\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    while not all(done) or t < 1000:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t+=1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], values[agent], log_probs[agent], reward[agent], done[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "    print(f'Finished episode {s}')\n",
    "\n",
    "Buffer.compute_advantages_and_returns()\n",
    "print(\"Finished Rnd Exploration\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=9']\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledSAC(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m brain\u001b[38;5;241m.\u001b[39mfine_tune_teacher(Buffer)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\SAC_Distillation\\DistilledSACAgent.py:186\u001b[0m, in \u001b[0;36mDistilledSAC.train\u001b[1;34m(self, steps, trajectories)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    185\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39mrsample()  \u001b[38;5;66;03m# Differentiable sampling for target Q\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     next_q1, next_q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_critics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     next_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(next_q1, next_q2)\n\u001b[0;32m    188\u001b[0m     bootstrapped_q \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m next_values\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\SAC_Distillation\\Nets.py:234\u001b[0m, in \u001b[0;36mSACNetWithDistillation.get_critics\u001b[1;34m(self, camera_obs, vector_obs, actions)\u001b[0m\n\u001b[0;32m    232\u001b[0m     vector_obs \u001b[38;5;241m=\u001b[39m vector_obs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    233\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution_pipeline(camera_obs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 234\u001b[0m fc_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconv_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_obs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m fc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfully_connected_pipeline(fc_input)\n\u001b[0;32m    236\u001b[0m critic_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fc_out, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "brain.fine_tune_teacher(Buffer)\n",
    "brain.train(1000,Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index is out of bounds for dimension with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# SAC optimization step\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m Buffer\u001b[38;5;241m.\u001b[39mcompute_advantages_and_returns()\n\u001b[0;32m     38\u001b[0m brain\u001b[38;5;241m.\u001b[39mfine_tune_teacher(Buffer)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\SAC_Distillation\\DistilledSACAgent.py:144\u001b[0m, in \u001b[0;36mDistilledSAC.train\u001b[1;34m(self, steps, trajectories)\u001b[0m\n\u001b[0;32m    141\u001b[0m samples_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msac_epochs):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m trajectories\u001b[38;5;241m.\u001b[39msample():\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m samples_processed \u001b[38;5;241m>\u001b[39m steps:\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\SAC_Distillation\\Trajectories.py:82\u001b[0m, in \u001b[0;36mExperienceBuffer.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m indices_generator \u001b[38;5;241m=\u001b[39m BatchSampler(\n\u001b[0;32m     71\u001b[0m     SubsetRandomSampler(\u001b[38;5;28mrange\u001b[39m(current_size)), \n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_batch_size, \n\u001b[0;32m     73\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_indices \u001b[38;5;129;01min\u001b[39;00m indices_generator:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_obs[batch_indices],\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_obs[batch_indices],\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_indices],\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_log_probs[batch_indices],\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_estimates[batch_indices],\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvantages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indices\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[batch_indices],\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_flags[batch_indices]\n\u001b[0;32m     85\u001b[0m     )\n",
      "\u001b[1;31mIndexError\u001b[0m: index is out of bounds for dimension with size 0"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "best_mean_reward = -np.inf\n",
    "not_improved = 0\n",
    "while steps < params['sac_distilled'].max_steps:\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    episode_reward = 0\n",
    "    while not all(done) or t < params['sac_distilled'].n_steps:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t+=1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], values[agent], log_probs[agent], reward[agent], done[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "        tot_reward = [reward[agent] for agent in agents if agent in reward.keys()]\n",
    "    obs_keys = list(obs.keys())\n",
    "    _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "    Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "    mean_reward = np.mean(tot_reward)\n",
    "    \n",
    "    steps += t\n",
    "\n",
    "    # SAC optimization step\n",
    "    brain.train(steps, Buffer)\n",
    "\n",
    "    Buffer.compute_advantages_and_returns()\n",
    "    brain.fine_tune_teacher(Buffer)\n",
    "    brain.optimizer = brain.adjust_lr(brain.optimizer, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    brain.optimizer_distill = brain.adjust_lr(brain.optimizer_distill, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    wandb.log({\"Mean Reward\": mean_reward, \"Steps\": steps})\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
