{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"SAC_Distillation\", entity=\"fede-\")\n",
    "wandb.config.update(params['sac_distilled'])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.config.update({\"device\": device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape,env.action_space(agents[0]).shape, params['sac_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledSAC(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(1, params['ppo_distilled'].seed_episodes + 1):\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    while not all(done) or t < params['ppo_distilled'].n_steps_random_exploration:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t+=1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "    print(f'Finished episode {s}')\n",
    "\n",
    "Buffer.compute_advantages_and_returns()\n",
    "print(\"Finished Rnd Exploration\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledSAC(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.fine_tune_teacher(Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "best_mean_reward = -np.inf\n",
    "not_improved = 0\n",
    "while steps < params['sac_distilled'].max_steps:\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    episode_reward = 0\n",
    "    while not all(done) or t < params['sac_distilled'].n_steps:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t += 1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "        tot_reward = [reward[agent] for agent in agents if agent in reward.keys()]\n",
    "    obs_keys = list(obs.keys())\n",
    "    _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "    Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "    mean_reward = np.mean(tot_reward)\n",
    "    \n",
    "    steps += t\n",
    "\n",
    "    # SAC optimization step\n",
    "    brain.train(steps, Buffer)\n",
    "    \n",
    "    brain.optimizer = brain.improv_lr(brain.optimizer, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    wandb.log({\"Mean Reward\": mean_reward, \"Steps\": steps})\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
