{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, time, json, pathlib\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "ENABLE_COMPILE = False\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fede\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:34: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from Hyperparameters import SAC_DISTILLED as H\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.TeacherModel import TeacherModel\n",
    "from SAC_Distillation.Trajectories import SAC_ExperienceBuffer\n",
    "\n",
    "SHORT_RUN = False\n",
    "if SHORT_RUN:\n",
    "    H[\"train_steps\"] = 150_000\n",
    "    H[\"warmup_steps\"] = 10_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gamma\": 0.99,\n",
      "  \"gae_lambda\": 0.95,\n",
      "  \"tau\": 0.005,\n",
      "  \"actor_lr\": 0.0003,\n",
      "  \"critic_lr\": 0.0003,\n",
      "  \"alpha_lr\": 0.0003,\n",
      "  \"distill_lr\": 0.0001,\n",
      "  \"rnd_lr\": 5e-05,\n",
      "  \"critic_updates\": 3,\n",
      "  \"actor_updates\": 2,\n",
      "  \"distill_coef\": 0.06,\n",
      "  \"distill_epochs\": 2,\n",
      "  \"distill_batch\": 256,\n",
      "  \"distill_temp\": 0.07,\n",
      "  \"distill_frames\": 50000,\n",
      "  \"buffer_size\": 1000000,\n",
      "  \"batch_size\": 512,\n",
      "  \"seed_episodes\": 1,\n",
      "  \"n_steps_random_exploration\": 10000,\n",
      "  \"noise_std\": 0.2,\n",
      "  \"smooth_clip\": 0.2,\n",
      "  \"rnd_update_proportion\": 0.05,\n",
      "  \"intrinsic_reward_coef\": 0.5,\n",
      "  \"intrinsic_reward_coef_final\": 0.15,\n",
      "  \"intrinsic_coef_decay_steps\": 1000000,\n",
      "  \"extrinsic_reward_coef\": 1.0,\n",
      "  \"max_steps\": 5000000,\n",
      "  \"policy_delay\": 2,\n",
      "  \"ema_alpha\": 0.01,\n",
      "  \"curiosity_coeff\": 0.1,\n",
      "  \"warmup_steps\": 100000,\n",
      "  \"use_drq\": true,\n",
      "  \"drq_pad\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "H.update({\n",
    "    \"use_drq\": True,\n",
    "    \"drq_pad\": 4,\n",
    "    \"intrinsic_reward_coef\":        0.5,\n",
    "    \"intrinsic_reward_coef_final\":  0.15,\n",
    "    \"intrinsic_coef_decay_steps\":   1_000_000,\n",
    "    \"warmup_steps\": 100_000,\n",
    "})\n",
    "print(json.dumps(H, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "# ---------- shared helpers ----------\n",
    "def _stable_unique(seq):\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "def _to_chw01(cam: np.ndarray) -> np.ndarray:\n",
    "    if cam.ndim != 3:\n",
    "        raise AssertionError(f\"Camera observation should be 3D (HWC or CHW), got {cam.shape}\")\n",
    "    if cam.shape[-1] in (1, 3, 4):  # HWC -> CHW\n",
    "        cam = np.transpose(cam, (2, 0, 1))\n",
    "    cam = cam.astype(np.float32, copy=False)\n",
    "    if cam.max() > 1.5:\n",
    "        cam = cam / 255.0\n",
    "    return cam\n",
    "\n",
    "def _to_vec1d(vec: np.ndarray) -> np.ndarray:\n",
    "    vec = np.asarray(vec)\n",
    "    if vec.ndim > 1:\n",
    "        vec = vec.reshape(-1)\n",
    "    return vec.astype(np.float32, copy=False)\n",
    "\n",
    "def _extract_agent_obs(obs, agent, *, cam_key=1, vec_keys=(0,2)):\n",
    "    if agent not in obs:\n",
    "        raise KeyError(f\"Agent {agent!r} not in obs keys: {list(obs.keys())[:8]}...\")\n",
    "    data = obs[agent]\n",
    "    if isinstance(data, dict) and \"observation\" in data:\n",
    "        data = data[\"observation\"]\n",
    "\n",
    "    if isinstance(data, dict) and (\"camera_obs\" in data and \"vector_obs\" in data):\n",
    "        cam = np.asarray(data[\"camera_obs\"])\n",
    "        vec = np.asarray(data[\"vector_obs\"])\n",
    "    else:\n",
    "        cam = np.asarray(data[cam_key])\n",
    "        parts = []\n",
    "        for k in vec_keys:\n",
    "            v = np.asarray(data[k])\n",
    "            if v.ndim > 1:\n",
    "                v = v.reshape(-1)\n",
    "            parts.append(v)\n",
    "        vec = np.concatenate(parts, axis=0) if len(parts) > 1 else parts[0]\n",
    "\n",
    "    cam = _to_chw01(cam)\n",
    "    vec = _to_vec1d(vec)\n",
    "    return cam, vec\n",
    "\n",
    "def _stack_from_per_agent(obs_dict, agent_ids, *, cam_key=1, vec_keys=(0,2)):\n",
    "    cams, vecs = [], []\n",
    "    for aid in agent_ids:\n",
    "        cam, vec = _extract_agent_obs(obs_dict, aid, cam_key=cam_key, vec_keys=vec_keys)\n",
    "        cams.append(cam); vecs.append(vec)\n",
    "    camera_obs = np.stack(cams, axis=0)  # [B,C,H,W]\n",
    "    maxd = max(v.shape[0] for v in vecs)\n",
    "    vecs = [np.pad(v, (0, maxd - v.shape[0])) if v.shape[0] != maxd else v for v in vecs]\n",
    "    vector_obs = np.stack(vecs, axis=0)  # [B,D]\n",
    "    return {\"camera_obs\": camera_obs, \"vector_obs\": vector_obs}\n",
    "\n",
    "def _to_rew(x, agent_ids):\n",
    "    if isinstance(x, dict):\n",
    "        arr = np.asarray([x.get(a, 0.0) for a in agent_ids], dtype=np.float32)\n",
    "    elif isinstance(x, (list, tuple, np.ndarray)):\n",
    "        arr = np.asarray(x, dtype=np.float32)\n",
    "    else:\n",
    "        arr = np.asarray([x], dtype=np.float32)\n",
    "    return arr.reshape(-1, 1)\n",
    "\n",
    "def _to_done_dict_false(agent_ids):\n",
    "    return {a: False for a in agent_ids}\n",
    "\n",
    "def _to_done(x, agent_ids):\n",
    "    if isinstance(x, dict):\n",
    "        arr = np.asarray([x.get(a, False) for a in agent_ids], dtype=bool)\n",
    "    elif isinstance(x, (list, tuple, np.ndarray)):\n",
    "        arr = np.asarray(x, dtype=bool)\n",
    "    else:\n",
    "        arr = np.asarray([x], dtype=bool)\n",
    "    return arr.reshape(-1, 1)\n",
    "\n",
    "# ---------- UnityParallelEnv adapter (PettingZoo Parallel) ----------\n",
    "class ParallelMultiDroneAdapter:\n",
    "    \"\"\"\n",
    "    Adapter for your UnityParallelEnv (PettingZoo Parallel).\n",
    "    Exposes a Gym-like API:\n",
    "      reset() -> (batched_obs, {})\n",
    "      step(batched_actions) -> (batched_obs, rewards[B,1], terminated[B,1], truncated[B,1], info)\n",
    "    \"\"\"\n",
    "    def __init__(self, env, *, cam_key=1, vec_keys=(0,2)):\n",
    "        self.env = env\n",
    "        self.cam_key = cam_key\n",
    "        self.vec_keys = vec_keys\n",
    "        self.agent_ids = None\n",
    "        self.num_agents = None\n",
    "        self._batched_action_space = None\n",
    "\n",
    "    # --- action space resolution (must be continuous Box for SAC) ---\n",
    "    def _resolve_single_action_space(self, agent_id):\n",
    "        # Try common patterns\n",
    "        sp = None\n",
    "        if hasattr(self.env, \"action_space\") and callable(getattr(self.env, \"action_space\", None)):\n",
    "            sp = self.env.action_space(agent_id)\n",
    "        elif hasattr(self.env, \"action_space\") and isinstance(self.env.action_space, dict):\n",
    "            sp = self.env.action_space.get(agent_id)\n",
    "        elif hasattr(self.env, \"action_spaces\"):\n",
    "            sp = self.env.action_spaces.get(agent_id)\n",
    "\n",
    "        if isinstance(sp, spaces.Box):\n",
    "            return sp\n",
    "\n",
    "        if isinstance(sp, spaces.Dict):\n",
    "            boxes = [v for v in sp.spaces.values() if isinstance(v, spaces.Box)]\n",
    "            if len(boxes) == 1:\n",
    "                return boxes[0]\n",
    "            raise AssertionError(\n",
    "                f\"Dict action space has {len(sp.spaces)} entries ({len(boxes)} Box). \"\n",
    "                \"Expose a single continuous Box or implement concatenation.\"\n",
    "            )\n",
    "\n",
    "        if isinstance(sp, spaces.Tuple):\n",
    "            boxes = [s for s in sp.spaces if isinstance(s, spaces.Box)]\n",
    "            if len(sp.spaces) == 1 and len(boxes) == 1:\n",
    "                return boxes[0]\n",
    "            raise AssertionError(\n",
    "                f\"Tuple action space has {len(sp.spaces)} entries (Box count: {len(boxes)}). \"\n",
    "                \"Expose a single continuous Box or implement concatenation.\"\n",
    "            )\n",
    "\n",
    "        if isinstance(sp, (spaces.Discrete, spaces.MultiDiscrete, spaces.MultiBinary)) or sp is None:\n",
    "            raise AssertionError(\n",
    "                f\"SAC requires a continuous Box action space, got {type(sp).__name__ if sp is not None else 'None'}.\"\n",
    "            )\n",
    "\n",
    "        raise AssertionError(f\"Unsupported per-agent action space type: {type(sp).__name__}\")\n",
    "\n",
    "    def _get_batched_action_space(self):\n",
    "        if self.agent_ids is None:\n",
    "            # Need one reset to know agents\n",
    "            _ = self.reset()\n",
    "        first = self.agent_ids[0]\n",
    "        box = self._resolve_single_action_space(first)\n",
    "\n",
    "        low  = np.repeat(box.low[None, ...], self.num_agents, axis=0)\n",
    "        high = np.repeat(box.high[None, ...], self.num_agents, axis=0)\n",
    "\n",
    "        batched = spaces.Box(low=low, high=high, dtype=box.dtype)\n",
    "        batched.shape = (self.num_agents, box.shape[0])\n",
    "\n",
    "        def _sample():\n",
    "            return np.stack([box.sample() for _ in range(self.num_agents)], axis=0)\n",
    "        batched.sample = _sample\n",
    "        return batched\n",
    "\n",
    "    # Allow both callable and property usage\n",
    "    def action_space(self):\n",
    "        return self._get_batched_action_space()\n",
    "    @property\n",
    "    def action_space_property(self):\n",
    "        return self._get_batched_action_space()\n",
    "\n",
    "    # --- Gym-like API built on your UnityParallelEnv signatures ---\n",
    "    def reset(self, **kwargs):\n",
    "        # Your UnityParallelEnv.reset() -> observations (dict), no info\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        if not isinstance(obs, dict):\n",
    "            raise TypeError(\"UnityParallelEnv.reset() must return a dict of per-agent observations.\")\n",
    "        if self.agent_ids is None:\n",
    "            self.agent_ids = _stable_unique(list(obs.keys())) if len(obs) > 0 else _stable_unique(getattr(self.env, \"agents\", []))\n",
    "            self.num_agents = len(self.agent_ids)\n",
    "\n",
    "        batched = _stack_from_per_agent(obs, self.agent_ids, cam_key=self.cam_key, vec_keys=self.vec_keys)\n",
    "        return batched, {}  # empty info for Gym-compat\n",
    "\n",
    "    def step(self, action_batched):\n",
    "        # Map (B, act_dim) -> per-agent dict\n",
    "        if isinstance(action_batched, dict):\n",
    "            action_dict = action_batched\n",
    "        else:\n",
    "            action_batched = np.asarray(action_batched)\n",
    "            assert action_batched.shape[0] == self.num_agents, f\"Expected {self.num_agents} actions, got {action_batched.shape}\"\n",
    "            action_dict = {aid: action_batched[i] for i, aid in enumerate(self.agent_ids)}\n",
    "\n",
    "        # Your UnityParallelEnv.step(actions) -> (obs, rewards, dones, infos)\n",
    "        next_obs, rewards, dones, infos = self.env.step(action_dict)\n",
    "\n",
    "        if not isinstance(next_obs, dict):\n",
    "            raise TypeError(\"UnityParallelEnv.step() must return per-agent observation dict.\")\n",
    "\n",
    "        batched_obs = _stack_from_per_agent(next_obs, self.agent_ids, cam_key=self.cam_key, vec_keys=self.vec_keys)\n",
    "        rew  = _to_rew(rewards, self.agent_ids)     # (B,1)\n",
    "        term = _to_done(dones,   self.agent_ids)    # (B,1)\n",
    "        # UnityParallelEnv has no truncations -> synthesize all False\n",
    "        trunc = _to_done(_to_done_dict_false(self.agent_ids), self.agent_ids)\n",
    "\n",
    "        # infos is already a dict (per agent / global); pass it through\n",
    "        return batched_obs, rew, term, trunc, infos\n",
    "\n",
    "    # passthroughs\n",
    "    def close(self): \n",
    "        return getattr(self.env, \"close\", lambda: None)()\n",
    "    def render(self, *a, **k):\n",
    "        return getattr(self.env, \"render\", lambda *a, **k: None)(*a, **k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    \"\"\"\n",
    "    Return a stable, deduplicated list of agent IDs.\n",
    "    Works for:\n",
    "      - PettingZoo-style envs with `env.agents`\n",
    "      - Unity/Gym wrappers exposing `num_agents` or agent names\n",
    "      - Falls back to a range of indices if no IDs are present\n",
    "    \"\"\"\n",
    "    # PettingZoo-style\n",
    "    if hasattr(env, \"agents\") and isinstance(getattr(env, \"agents\"), (list, tuple)):\n",
    "        return list(dict.fromkeys(env.agents))  # stable-unique\n",
    "\n",
    "    # Some Unity wrappers expose number of agents\n",
    "    if hasattr(env, \"num_agents\") and isinstance(env.num_agents, (int, np.integer)):\n",
    "        return list(range(int(env.num_agents)))\n",
    "\n",
    "    # Try to infer from observation space if available\n",
    "    if hasattr(env, \"observation_space\") and hasattr(env.observation_space, \"n\"):\n",
    "        return list(range(int(env.observation_space.n)))\n",
    "\n",
    "    # Last resort: single agent\n",
    "    return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "\n",
    "env = UE(file_name=\"Env/Level1/DroneFlightv1\", seed=1)\n",
    "env = UPZBE(env)\n",
    "env = ParallelMultiDroneAdapter(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['camera_obs', 'vector_obs'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "obs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeacherPretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = TeacherModel().to(device)\n",
    "if ENABLE_COMPILE:\n",
    "    try:\n",
    "        teacher = torch.compile(teacher, mode=\"reduce-overhead\", fullgraph=False)\n",
    "    except Exception as e:\n",
    "        print(\"compile skipped:\", e)\n",
    "\n",
    "opt_t = torch.optim.AdamW(teacher.head.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def gather_random_frames(env, n_steps=2000):\n",
    "    assert env is not None, \"Instantiate env before running\"\n",
    "    frames = []\n",
    "    with torch.no_grad():\n",
    "        obs = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            a = env.action_space.sample()\n",
    "            o2, r, done, info = env.step(a)\n",
    "            cam = o2[\"camera_obs\"]\n",
    "            frames.append(torch.from_numpy(cam))\n",
    "            if np.any(done):\n",
    "                obs = env.reset()\n",
    "            else:\n",
    "                obs = o2\n",
    "\n",
    "    return torch.cat(frames, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Unsupported per-agent action space type: Box",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[4], line 155\u001b[0m, in \u001b[0;36mParallelMultiDroneAdapter.action_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maction_space\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batched_action_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 140\u001b[0m, in \u001b[0;36mParallelMultiDroneAdapter._get_batched_action_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    139\u001b[0m first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_ids[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 140\u001b[0m box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_single_action_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m low  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(box\u001b[38;5;241m.\u001b[39mlow[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    143\u001b[0m high \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(box\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 133\u001b[0m, in \u001b[0;36mParallelMultiDroneAdapter._resolve_single_action_space\u001b[1;34m(self, agent_id)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sp, (spaces\u001b[38;5;241m.\u001b[39mDiscrete, spaces\u001b[38;5;241m.\u001b[39mMultiDiscrete, spaces\u001b[38;5;241m.\u001b[39mMultiBinary)) \u001b[38;5;129;01mor\u001b[39;00m sp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC requires a continuous Box action space, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sp)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39msp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported per-agent action space type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(sp)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Unsupported per-agent action space type: Box"
     ]
    }
   ],
   "source": [
    "print(env.action_space().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Only Box action spaces are supported in this adapter.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m replay \u001b[38;5;241m=\u001b[39m SAC_ExperienceBuffer(camera_obs_dim\u001b[38;5;241m=\u001b[39mobs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamera_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, vector_obs_dim\u001b[38;5;241m=\u001b[39mobs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,action_dim\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m(), params\u001b[38;5;241m=\u001b[39mH)\n",
      "Cell \u001b[1;32mIn[4], line 171\u001b[0m, in \u001b[0;36m_ParallelMultiDroneAdapter.action_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Some envs expose .action_space(agent) vs dict\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     a_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space[first]\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a_space, spaces\u001b[38;5;241m.\u001b[39mBox), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Box action spaces are supported in this adapter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m low  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(a_space\u001b[38;5;241m.\u001b[39mlow[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    173\u001b[0m high \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(a_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Only Box action spaces are supported in this adapter."
     ]
    }
   ],
   "source": [
    "replay = SAC_ExperienceBuffer(camera_obs_dim=obs[\"camera_obs\"].shape, vector_obs_dim=obs[\"vector_obs\"].shape,action_dim=env., params=H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
