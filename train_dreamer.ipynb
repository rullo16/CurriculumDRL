{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from Dreamer.agent import Dreamer\n",
    "from Dreamer.memory import ExperienceReplay\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1']\n"
     ]
    }
   ],
   "source": [
    "agents = [agent for agent in env.agents]\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.   10.    0.   -8.95 20.   25.77  0.   10.    0.   -8.95 20.   25.77]\n",
      "(6,)\n",
      "(1, 84, 84)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[agents[0]][2])\n",
    "print(env.action_space(agents[0]).shape)\n",
    "# print(env.observation_space(agents[0])[0].shape)\n",
    "print(env.observation_space(agents[0])[1].shape)\n",
    "print(env.observation_space(agents[0])[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    'dreamer': SimpleNamespace(**{\n",
    "        'belief_size': 200,\n",
    "        'state_size': 30,\n",
    "        'hidden_size': 200,\n",
    "        'embedding_size': 1024,\n",
    "        'dense_act': 'relu',\n",
    "        'symbolic': False,\n",
    "        'camera_obs': env.observation_space(agents[0])[1].shape,\n",
    "        'vector_obs': env.observation_space(agents[0])[2].shape[0],\n",
    "        'action_size': env.action_space(agents[0]).shape[0],\n",
    "        'cnn_act': 'relu',\n",
    "        'pcont': True,\n",
    "        'world_lr': 1e-3,\n",
    "        'actor_lr': 8e-5,\n",
    "        'value_lr': 8e-5,\n",
    "        'free_nats': 3.0,\n",
    "        'device': 'cuda',\n",
    "        'reward_scale': 1.0,\n",
    "        'pcont_scale': 10.0,\n",
    "        'discount': 0.99,\n",
    "        'disclam': 0.95,\n",
    "        'grad_clip_norm': 100.0,\n",
    "        'planning_horizon': 15,\n",
    "        'expl_amount': 0.3,\n",
    "        'with_logprob': False,\n",
    "        'temp': 0.1,\n",
    "        'batch_size': 50,\n",
    "        'bit_depth': 5,\n",
    "        'experience_size': 100000,\n",
    "        'seed_episodes': 10,\n",
    "        'collect_interval': 100,\n",
    "        'chunk_size': 50,\n",
    "    }),\n",
    "}\n",
    "\n",
    "n_episodes = 1000\n",
    "n_steps = 10000\n",
    "n_steps_rnd_exploration = 10\n",
    "net_update= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = HYPERPARAMS['dreamer']\n",
    "brain = Dreamer(\n",
    "\talgo_name='dreamer',\n",
    "\tdeter_dim=params.belief_size,\n",
    "\tstoc_dim=params.state_size,\n",
    "\tmlp_dim=params.hidden_size,\n",
    "\tembedding_dim=params.embedding_size,\n",
    "\tobs_shape=params.camera_obs,\n",
    "\taction_dim=params.action_size,\n",
    "\tmlp_layer=2,\n",
    "\tworld_lr=params.world_lr,\n",
    "\tactor_lr=params.actor_lr,\n",
    "\tvalue_lr=params.value_lr,\n",
    "\tgrad_clip_norm=params.grad_clip_norm,\n",
    "\tweight_decay=0.0,\n",
    "\tactor_ent=3.0,\n",
    "\tfree_nats=params.free_nats,\n",
    "\tcoef_pred=1.0,\n",
    "\tcoef_dyn=1.0,\n",
    "\tcoef_rep=1.0,\n",
    "\timag_length=params.planning_horizon,\n",
    "\tdevice=params.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ExperienceReplay(params.experience_size, params.symbolic, params.camera_obs, params.vector_obs, params.action_size, params.bit_depth, params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents():\n",
    "    return [agent for agent in env.agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1\n",
      "Finished Rnd Exploration\n"
     ]
    }
   ],
   "source": [
    "# for s in range(1, params.seed_episodes + 1):\n",
    "for s in range(1, 2):\n",
    "    obs, done, t = env.reset(), [False, False, False], 0\n",
    "    while not all(done) or t < n_steps_rnd_exploration:\n",
    "        actions = {}\n",
    "        agents = relocate_agents()\n",
    "        for agent in agents:\n",
    "            actions[agent] = env.action_space(agent).sample()\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            if isinstance(obs[agent], list):\n",
    "                D.add(np.array(obs[agent][1]), np.array(obs[agent][2]), actions[agent], reward[agent], done[agent])\n",
    "            else:\n",
    "                D.add(np.array(obs[agent]['observation'][1]),np.array(obs[agent]['observation'][2]), actions[agent], reward[agent], done[agent])\n",
    "        t += 1\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "    print(f'Finished episode {s}')\n",
    "\n",
    "print(\"Finished Rnd Exploration\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)\n",
    "agents = relocate_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransitionModel.forward() missing 2 required positional arguments: 'observations' and 'nonterminals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39msample(params\u001b[38;5;241m.\u001b[39mbatch_size, params\u001b[38;5;241m.\u001b[39mchunk_size)\n\u001b[0;32m      3\u001b[0m agents \u001b[38;5;241m=\u001b[39m relocate_agents()\n\u001b[1;32m----> 4\u001b[0m loss_info \u001b[38;5;241m=\u001b[39m \u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mloss_info))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Code\\Dreamer\\agent.py:202\u001b[0m, in \u001b[0;36mDreamer.update\u001b[1;34m(self, data, gradient_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    200\u001b[0m     p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m imag_beliefs, imag_states, imag_ac_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_latent_imagination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeliefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_logprob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_logprob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss_actor(imag_beliefs, imag_states, imag_ac_logps)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Code\\Dreamer\\agent.py:158\u001b[0m, in \u001b[0;36mDreamer._latent_imagination\u001b[1;34m(self, beliefs, posterior_states, with_logprob)\u001b[0m\n\u001b[0;32m    150\u001b[0m imag_action, imag_ac_logp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_model(\n\u001b[0;32m    151\u001b[0m     imag_beliefs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(),\n\u001b[0;32m    152\u001b[0m     imag_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(),\n\u001b[0;32m    153\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     with_logprob\u001b[38;5;241m=\u001b[39mwith_logprob\n\u001b[0;32m    155\u001b[0m )\n\u001b[0;32m    156\u001b[0m imag_action \u001b[38;5;241m=\u001b[39m imag_action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 158\u001b[0m imag_belief, imag_state, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimag_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimag_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimag_beliefs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m imag_beliefs\u001b[38;5;241m.\u001b[39mappend(imag_belief\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    160\u001b[0m imag_states\u001b[38;5;241m.\u001b[39mappend(imag_state\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TransitionModel.forward() missing 2 required positional arguments: 'observations' and 'nonterminals'"
     ]
    }
   ],
   "source": [
    "def replay_generator(D, batch_size, chunk_size):\n",
    "    while True:\n",
    "        yield D.sample(batch_size, chunk_size)\n",
    "\n",
    "replay_iter = replay_generator(D, params.batch_size, params.chunk_size)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    agents = relocate_agents()\n",
    "    metrics = brain.update(replay_iter)\n",
    "    losses = tuple(zip(*metrics))\n",
    "    with torch.no_grad():\n",
    "        obs, tot_reward = env.reset(), 0\n",
    "        belief = torch.zeros(1, params.belief_size).to(params.device)\n",
    "        posterior_state = torch.zeros(1, params.state_size).to(params.device)\n",
    "        action = torch.zeros(1, params.action_size).to(params.device)\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            agents = relocate_agents()\n",
    "            actions={}\n",
    "            for agent in agents:\n",
    "                if isinstance(obs[agent], list):\n",
    "                    belief, posterior_state = brain.infer_state(obs[agent][0], action, belief, posterior_state)\n",
    "                else:\n",
    "                    belief, posterior_state = brain.infer_state(obs[agent]['observation'][0], action, belief, posterior_state)\n",
    "                action = brain.select_action((belief, posterior_state), deterministic=True)\n",
    "                actions[agent] = action.argmax().detach().cpu().item()\n",
    "\n",
    "            next_obs, reward, done,_ = env.step(actions)\n",
    "            tot_reward += sum(reward.values())\n",
    "            agents = relocate_agents()\n",
    "            for agent in agents:\n",
    "                if agent not in obs.keys() or agent not in actions.keys():\n",
    "                    continue\n",
    "                if isinstance(obs[agent], list):\n",
    "                    camera = np.array(obs[agent][0])\n",
    "                    camera = camera[0]\n",
    "                    camera = camera[None, ...]  # keep shape (1,84,84)\n",
    "                    D.add(camera, actions[agent], reward[agent], done[agent])\n",
    "                else:\n",
    "                    camera = np.array(obs[agent]['observation'][0])\n",
    "                    camera = camera[0]\n",
    "                    camera = camera[None, ...]  # keep shape (1,84,84)\n",
    "                    D.add(camera, actions[agent], reward[agent], done[agent])\n",
    "            if len(done.values())== 1 and all(done.values()):\n",
    "                next_obs = env.reset()\n",
    "            obs = next_obs\n",
    "        \n",
    "    print(f\"Episode {episode} finished with reward {tot_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
