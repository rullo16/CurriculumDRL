{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Optuna for PPO and SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from PPO_Distillation.DistilledPPOAgent import DistilledPPO\n",
    "from PPO_Distillation.Trajectories import PPO_ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import SAC_ExperienceBuffer\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import optuna\n",
    "import optunahub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999, step=0.001)\n",
    "    entropy_coef = trial.suggest_float('entropy_coef', 0.01, 0.1, step=0.01)\n",
    "    value_loss_coef = trial.suggest_float('value_loss_coef', 0.1, 1.0, step=0.1)\n",
    "    clip_grad_norm = trial.suggest_float('clip_grad_norm', 0.1, 1.0, step=0.1)\n",
    "    action_std = trial.suggest_float('action_std', 0.1, 1.0, step=0.1)\n",
    "\n",
    "    # Update the hyperparameters in the configuration\n",
    "    params['ppo_distilled'].lr = lr\n",
    "    params['ppo_distilled'].gamma = gamma\n",
    "    params['ppo_distilled'].entropy_coef = entropy_coef\n",
    "    params['ppo_distilled'].value_loss_coef = value_loss_coef\n",
    "    params['ppo_distilled'].clip_grad_norm = clip_grad_norm\n",
    "    params['ppo_distilled'].action_std = action_std\n",
    "\n",
    "    # Initialize the agent\n",
    "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "    env = UPZBE(env)\n",
    "    agents = relocate_agents(env)\n",
    "    brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "    Buffer = PPO_ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "\n",
    "    # Training loop\n",
    "    steps = 0\n",
    "    while steps < 100000:\n",
    "        obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "        while not all(done) or t < params['ppo_distilled'].n_steps:\n",
    "            actions, log_probs, values = {}, {}, {}\n",
    "            agents = relocate_agents(env)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "                t += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(actions)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            done = [done[agent] for agent in agents if agent in done]\n",
    "            tot_reward = [reward[agent] for agent in agents if agent in reward]\n",
    "        \n",
    "        obs_keys = list(obs.keys())\n",
    "        _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "        Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "        mean_reward = np.mean(tot_reward)\n",
    "        \n",
    "        steps += t\n",
    "\n",
    "        brain.train(steps, Buffer)\n",
    "        Buffer.compute_advantages_and_returns()\n",
    "        brain.optimizer = brain.improv_lr(brain.optimizer, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "        brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999, step=0.001)\n",
    "    entropy_coef = trial.suggest_float('entropy_coef', 0.01, 0.1, step=0.01)\n",
    "    value_loss_coef = trial.suggest_float('value_loss_coef', 0.1, 1.0, step=0.1)\n",
    "    clip_grad_norm = trial.suggest_float('clip_grad_norm', 0.1, 1.0, step=0.1)\n",
    "    action_std = trial.suggest_float('action_std', 0.1, 1.0, step=0.1)\n",
    "\n",
    "    # Update the hyperparameters in the configuration\n",
    "    params['sac_distilled'].lr = lr\n",
    "    params['sac_distilled'].gamma = gamma\n",
    "    params['sac_distilled'].entropy_coef = entropy_coef\n",
    "    params['sac_distilled'].value_loss_coef = value_loss_coef\n",
    "    params['sac_distilled'].clip_grad_norm = clip_grad_norm\n",
    "    params['sac_distilled'].action_std = action_std\n",
    "\n",
    "    # Initialize the agent\n",
    "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "    env = UPZBE(env)\n",
    "    agents = relocate_agents(env)\n",
    "    brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])\n",
    "    Buffer = PPO_ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])\n",
    "\n",
    "    # Training loop\n",
    "    steps = 0\n",
    "    while steps < 100000:\n",
    "        obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "        while not all(done) or t < params['sac_distilled'].n_steps:\n",
    "            actions, log_probs, values = {}, {}, {}\n",
    "            agents = relocate_agents(env)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "                t += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(actions)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            done = [done[agent] for agent in agents if agent in done]\n",
    "            tot_reward = [reward[agent] for agent in agents if agent in reward]\n",
    "        \n",
    "        obs_keys = list(obs.keys())\n",
    "        _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "        Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "        mean_reward = np.mean(tot_reward)\n",
    "        \n",
    "        steps += t\n",
    "\n",
    "        brain.train(steps, Buffer)\n",
    "        Buffer.compute_advantages_and_returns()\n",
    "        brain.optimizer = brain.improv_lr(brain.optimizer, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "        brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Optuna for PPO and SAC Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = optunahub.load_module(package='samplers/auto_sampler')\n",
    "best_params = {}\n",
    "ppo_study = optuna.create_study(direction='maximize', sampler=module.AutoSampler(), study_name='ppo_distillation')\n",
    "sac_study = optuna.create_study(direction='maximize', sampler=module.AutoSampler(), study_name='sac_distillation')\n",
    "ppo_study.optimize(ppo_objective, n_trials=10)\n",
    "ppo_best_params = ppo_study.best_params\n",
    "sac_study.optimize(sac_objective, n_trials=10)\n",
    "sac_best_params = sac_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Best results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataframes for the best parameters\n",
    "ppo_best_params_df = pd.DataFrame(list(ppo_best_params.items()), columns=['Parameter', 'Value'])\n",
    "sac_best_params_df = pd.DataFrame(list(sac_best_params.items()), columns=['Parameter', 'Value'])\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Best PPO Parameters:\")\n",
    "display(ppo_best_params_df)\n",
    "\n",
    "print(\"Best SAC Parameters:\")\n",
    "display(sac_best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best PPO parameters\n",
    "for param, value in ppo_best_params.items():\n",
    "    params['ppo_distilled'][param] = value\n",
    "\n",
    "# Save the best SAC parameters\n",
    "for param, value in sac_best_params.items():\n",
    "    params['sac_distilled'][param] = value\n",
    "\n",
    "# Save the updated parameters to the Hyperparams module\n",
    "with open('Hyperparameters.py', 'w') as f:\n",
    "    f.write(f\"HYPERPARAMS = {params}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
