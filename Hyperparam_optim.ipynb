{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Optuna for PPO and SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from PPO_Distillation.DistilledPPOAgent import DistilledPPO\n",
    "from PPO_Distillation.Trajectories import PPO_ExperienceBuffer\n",
    "from Hyperparameters import HYPERPARAMS as params\n",
    "from SAC_Distillation.DistilledSACAgent import DistilledSAC\n",
    "from SAC_Distillation.Trajectories import SAC_ExperienceBuffer\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import optuna\n",
    "import optunahub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999, step=0.001)\n",
    "    entropy_coef = trial.suggest_float('entropy_coef', 0.01, 0.1, step=0.01)\n",
    "    value_loss_coef = trial.suggest_float('value_loss_coef', 0.1, 1.0, step=0.1)\n",
    "    clip_grad_norm = trial.suggest_float('clip_grad_norm', 0.1, 1.0, step=0.1)\n",
    "    action_std = trial.suggest_float('action_std', 0.1, 1.0, step=0.1)\n",
    "\n",
    "    # Update the hyperparameters in the configuration\n",
    "    params['ppo_distilled'].lr = lr\n",
    "    params['ppo_distilled'].gamma = gamma\n",
    "    params['ppo_distilled'].entropy_coef = entropy_coef\n",
    "    params['ppo_distilled'].value_loss_coef = value_loss_coef\n",
    "    params['ppo_distilled'].clip_grad_norm = clip_grad_norm\n",
    "    params['ppo_distilled'].action_std = action_std\n",
    "\n",
    "    # Initialize the agent\n",
    "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "    env = UPZBE(env)\n",
    "    agents = relocate_agents(env)\n",
    "    brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "    Buffer = PPO_ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "\n",
    "    # Training loop\n",
    "    steps = 0\n",
    "    while steps < 100000:\n",
    "        obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "        while not all(done) or t < params['ppo_distilled'].n_steps:\n",
    "            actions, log_probs, values = {}, {}, {}\n",
    "            agents = relocate_agents(env)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "                t += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(actions)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            done = [done[agent] for agent in agents if agent in done]\n",
    "            tot_reward = [reward[agent] for agent in agents if agent in reward]\n",
    "        \n",
    "        obs_keys = list(obs.keys())\n",
    "        _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "        Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "        mean_reward = np.mean(tot_reward)\n",
    "        \n",
    "        steps += t\n",
    "\n",
    "        brain.train(steps, Buffer)\n",
    "        Buffer.compute_advantages_and_returns()\n",
    "        brain.optimizer = brain.improv_lr(brain.optimizer, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "        brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999, step=0.001)\n",
    "    entropy_coef = trial.suggest_float('entropy_coef', 0.01, 0.1, step=0.01)\n",
    "    value_loss_coef = trial.suggest_float('value_loss_coef', 0.1, 1.0, step=0.1)\n",
    "    clip_grad_norm = trial.suggest_float('clip_grad_norm', 0.1, 1.0, step=0.1)\n",
    "    action_std = trial.suggest_float('action_std', 0.1, 1.0, step=0.1)\n",
    "\n",
    "    # Update the hyperparameters in the configuration\n",
    "    params['sac_distilled'].lr = lr\n",
    "    params['sac_distilled'].gamma = gamma\n",
    "    params['sac_distilled'].entropy_coef = entropy_coef\n",
    "    params['sac_distilled'].value_loss_coef = value_loss_coef\n",
    "    params['sac_distilled'].clip_grad_norm = clip_grad_norm\n",
    "    params['sac_distilled'].action_std = action_std\n",
    "\n",
    "    # Initialize the agent\n",
    "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "    env = UPZBE(env)\n",
    "    agents = relocate_agents(env)\n",
    "    brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])\n",
    "    Buffer = PPO_ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['sac_distilled'])\n",
    "\n",
    "    # Training loop\n",
    "    steps = 0\n",
    "    while steps < 100000:\n",
    "        obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "        while not all(done) or t < params['sac_distilled'].n_steps:\n",
    "            actions, log_probs, values = {}, {}, {}\n",
    "            agents = relocate_agents(env)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "                t += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(actions)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            done = [done[agent] for agent in agents if agent in done]\n",
    "            tot_reward = [reward[agent] for agent in agents if agent in reward]\n",
    "        \n",
    "        obs_keys = list(obs.keys())\n",
    "        _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "        Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "        mean_reward = np.mean(tot_reward)\n",
    "        \n",
    "        steps += t\n",
    "\n",
    "        brain.train(steps, Buffer)\n",
    "        Buffer.compute_advantages_and_returns()\n",
    "        brain.optimizer = brain.improv_lr(brain.optimizer, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "        brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['sac_distilled'].lr, steps, params['sac_distilled'].n_steps)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Optuna for PPO and SAC Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-27 14:55:39,311] A new study created in memory with name: ppo_distillation\n",
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "[I 2025-02-27 15:16:45,820] Trial 0 finished with value: -0.3754000663757324 and parameters: {'lr': 4.506163037934655e-05, 'gamma': 0.916, 'entropy_coef': 0.08, 'value_loss_coef': 0.30000000000000004, 'clip_grad_norm': 0.4, 'action_std': 0.5}. Best is trial 0 with value: -0.3754000663757324.\n",
      "C:\\Users\\rullo\\AppData\\Local\\optunahub\\cache\\api.github.com\\optuna\\optunahub-registry\\main\\package\\samplers/auto_sampler\\_sampler.py:184: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  return GPSampler(seed=seed)\n",
      "[I 2025-02-27 15:37:57,677] Trial 1 finished with value: -0.20873336493968964 and parameters: {'lr': 1.0293848976014494e-05, 'gamma': 0.902, 'entropy_coef': 0.01, 'value_loss_coef': 0.6, 'clip_grad_norm': 0.8, 'action_std': 0.4}. Best is trial 1 with value: -0.20873336493968964.\n",
      "[W 2025-02-28 14:29:57,824] Trial 2 failed with parameters: {'lr': 1.5569468678049464e-05, 'gamma': 0.932, 'entropy_coef': 0.05, 'value_loss_coef': 0.4, 'clip_grad_norm': 0.4, 'action_std': 0.30000000000000004} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\rullo\\AppData\\Local\\Temp\\ipykernel_15004\\3451996276.py\", line 39, in ppo_objective\n",
      "    obs, reward, done, _ = env.step(actions)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\envs\\unity_parallel_env.py\", line 47, in step\n",
      "    self._step()\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\envs\\unity_pettingzoo_base_env.py\", line 187, in _step\n",
      "    self._env.step()\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py\", line 353, in step\n",
      "    self._update_state(rl_output)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py\", line 306, in _update_state\n",
      "    self._env_state[brain_name] = steps_from_proto(\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py\", line 332, in steps_from_proto\n",
      "    _process_maybe_compressed_observation(\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py\", line 259, in _process_maybe_compressed_observation\n",
      "    batched_visual = [\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py\", line 260, in <listcomp>\n",
      "    _observation_to_np_array(agent_obs.observations[obs_index], shape)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py\", line 236, in _observation_to_np_array\n",
      "    img = process_pixels(\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py\", line 305, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py\", line 126, in process_pixels\n",
      "    np.moveaxis(np.array(image, dtype=np.float32) / 255.0, -1, 0)\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-28 14:29:57,866] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ppo_study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mAutoSampler(), study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_distillation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# sac_study = optuna.create_study(direction='maximize', sampler=module.AutoSampler(), study_name='sac_distillation')\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mppo_study\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m ppo_best_params \u001b[38;5;241m=\u001b[39m ppo_study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# sac_study.optimize(sac_objective, n_trials=10)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# sac_best_params = sac_study.best_params\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m, in \u001b[0;36mppo_objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     36\u001b[0m     actions[agent], log_probs[agent], values[agent] \u001b[38;5;241m=\u001b[39m brain\u001b[38;5;241m.\u001b[39mget_action(obs1, obs2)\n\u001b[0;32m     37\u001b[0m     t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 39\u001b[0m obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m obs:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\envs\\unity_parallel_env.py:47\u001b[0m, in \u001b[0;36mUnityParallelEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Agent cleanup and sorting\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_agents()\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\envs\\unity_pettingzoo_base_env.py:187\u001b[0m, in \u001b[0;36mUnityPettingzooBaseEnv._step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m behavior_name, actions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_action\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(behavior_name, actions)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_states()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m behavior_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mbehavior_specs\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py:353\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_behavior_specs(outputs)\n\u001b[0;32m    352\u001b[0m rl_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mrl_output\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_actions\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py:306\u001b[0m, in \u001b[0;36mUnityEnvironment._update_state\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brain_name \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39magentInfos:\n\u001b[0;32m    305\u001b[0m     agent_info_list \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39magentInfos[brain_name]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_state[brain_name] \u001b[38;5;241m=\u001b[39m \u001b[43msteps_from_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_info_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_specs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbrain_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_state[brain_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    311\u001b[0m         DecisionSteps\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_specs[brain_name]),\n\u001b[0;32m    312\u001b[0m         TerminalSteps\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_specs[brain_name]),\n\u001b[0;32m    313\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py:332\u001b[0m, in \u001b[0;36msteps_from_proto\u001b[1;34m(agent_info_list, behavior_spec)\u001b[0m\n\u001b[0;32m    329\u001b[0m is_visual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(observation_spec\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_visual:\n\u001b[0;32m    331\u001b[0m     decision_obs_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 332\u001b[0m         \u001b[43m_process_maybe_compressed_observation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobs_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision_agent_info_list\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     )\n\u001b[0;32m    336\u001b[0m     terminal_obs_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    337\u001b[0m         _process_maybe_compressed_observation(\n\u001b[0;32m    338\u001b[0m             obs_index, observation_spec, terminal_agent_info_list\n\u001b[0;32m    339\u001b[0m         )\n\u001b[0;32m    340\u001b[0m     )\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py:259\u001b[0m, in \u001b[0;36m_process_maybe_compressed_observation\u001b[1;34m(obs_index, observation_spec, agent_info_list)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, shape[\u001b[38;5;241m0\u001b[39m], shape[\u001b[38;5;241m1\u001b[39m], shape[\u001b[38;5;241m2\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m     batched_visual \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    260\u001b[0m         _observation_to_np_array(agent_obs\u001b[38;5;241m.\u001b[39mobservations[obs_index], shape)\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m agent_obs \u001b[38;5;129;01min\u001b[39;00m agent_info_list\n\u001b[0;32m    262\u001b[0m     ]\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# Try to get a more useful error message\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     _check_observations_match_spec(obs_index, observation_spec, agent_info_list)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py:260\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, shape[\u001b[38;5;241m0\u001b[39m], shape[\u001b[38;5;241m1\u001b[39m], shape[\u001b[38;5;241m2\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     batched_visual \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 260\u001b[0m         \u001b[43m_observation_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_obs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m agent_obs \u001b[38;5;129;01min\u001b[39;00m agent_info_list\n\u001b[0;32m    262\u001b[0m     ]\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# Try to get a more useful error message\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     _check_observations_match_spec(obs_index, observation_spec, agent_info_list)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py:236\u001b[0m, in \u001b[0;36m_observation_to_np_array\u001b[1;34m(obs, expected_shape)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pixels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompressed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompressed_channel_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Compare decompressed image size to observation shape and make sure they match\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape):\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_utils.py:126\u001b[0m, in \u001b[0;36mprocess_pixels\u001b[1;34m(image_bytes, expected_channels, mappings)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Normally Image loads lazily, load() forces it to do loading in the timer scope.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     image\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    125\u001b[0m image_arrays\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 126\u001b[0m     np\u001b[38;5;241m.\u001b[39mmoveaxis(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    127\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Look for the next header, starting from the current stream location\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "module = optunahub.load_module(package='samplers/auto_sampler')\n",
    "best_params = {}\n",
    "ppo_study = optuna.create_study(direction='maximize', sampler=module.AutoSampler(), study_name='ppo_distillation')\n",
    "# sac_study = optuna.create_study(direction='maximize', sampler=module.AutoSampler(), study_name='sac_distillation')\n",
    "ppo_study.optimize(ppo_objective, n_trials=10)\n",
    "ppo_best_params = ppo_study.best_params\n",
    "# sac_study.optimize(sac_objective, n_trials=10)\n",
    "# sac_best_params = sac_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Best results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppo_best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dataframes for the best parameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ppo_best_params_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(\u001b[43mppo_best_params\u001b[49m\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sac_best_params_df = pd.DataFrame(list(sac_best_params.items()), columns=['Parameter', 'Value'])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display the dataframes\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest PPO Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ppo_best_params' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataframes for the best parameters\n",
    "ppo_best_params_df = pd.DataFrame(list(ppo_best_params.items()), columns=['Parameter', 'Value'])\n",
    "# sac_best_params_df = pd.DataFrame(list(sac_best_params.items()), columns=['Parameter', 'Value'])\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"Best PPO Parameters:\")\n",
    "display(ppo_best_params_df)\n",
    "\n",
    "# print(\"Best SAC Parameters:\")\n",
    "# display(sac_best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best PPO parameters\n",
    "for param, value in ppo_best_params.items():\n",
    "    params['ppo_distilled'][param] = value\n",
    "\n",
    "# Save the best SAC parameters\n",
    "# for param, value in sac_best_params.items():\n",
    "#     params['sac_distilled'][param] = value\n",
    "\n",
    "# Save the updated parameters to the Hyperparams module\n",
    "with open('Hyperparameters.py', 'w') as f:\n",
    "    f.write(f\"HYPERPARAMS = {params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
