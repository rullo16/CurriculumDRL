{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3851f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib as plt\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "\n",
    "from MPC.MPC_Agent import MultiAgentMPC, MPCConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd511abe",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271e3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_obs(obs, agent, cam_key=1, vec_keys=[0, 2]):\n",
    "    \"\"\"\n",
    "    Extract observation data for an agent from the environment.\n",
    "    \"\"\"\n",
    "    if agent not in obs:\n",
    "        raise KeyError(f\"Agent {agent!r} not found in observations\")\n",
    "    \n",
    "    data = obs[agent]\n",
    "    if isinstance(data, dict) and \"observation\" in data:\n",
    "        data = data[\"observation\"]\n",
    "    \n",
    "    # Extract camera and vector observations\n",
    "    if isinstance(data, dict) and (\"camera_obs\" in data and \"vector_obs\" in data):\n",
    "        cam = np.asarray(data[\"camera_obs\"])\n",
    "        vec = np.asarray(data[\"vector_obs\"])\n",
    "        if vec.ndim > 1:\n",
    "            vec = vec.reshape(-1)\n",
    "    else:\n",
    "        cam = np.asarray(data[cam_key])\n",
    "        v0 = np.asarray(data[vec_keys[0]]).reshape(-1)\n",
    "        v1 = np.asarray(data[vec_keys[1]]).reshape(-1)\n",
    "        vec = np.concatenate([v0, v1], axis=0)\n",
    "    \n",
    "    # Process camera\n",
    "    if cam.ndim != 3:\n",
    "        raise AssertionError(f\"Camera must be 3D, got shape {cam.shape}\")\n",
    "    \n",
    "    if cam.shape[-1] in (1, 3, 4):\n",
    "        cam = np.transpose(cam, (2, 0, 1))\n",
    "    \n",
    "    cam = cam.astype(np.float32, copy=False)\n",
    "    if cam.max() > 1.5:\n",
    "        cam = cam / 255.0\n",
    "    \n",
    "    vec = vec.astype(np.float32, copy=False)\n",
    "    \n",
    "    return cam, vec\n",
    "\n",
    "\n",
    "def relocate_agents(env):\n",
    "    \"\"\"Get sorted list of agent IDs\"\"\"\n",
    "    return sorted(list(env.agents))\n",
    "\n",
    "\n",
    "def extract_state_from_obs(vector_obs: np.ndarray, state_dim: int = 12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract state representation for MPC from vector observations.\n",
    "    \n",
    "    Assuming vector_obs contains:\n",
    "    - Position (3D)\n",
    "    - Velocity (3D)\n",
    "    - Orientation (3D)\n",
    "    - Angular velocity (3D)\n",
    "    - Additional features...\n",
    "    \"\"\"\n",
    "    # Take first state_dim elements\n",
    "    state = vector_obs[:state_dim]\n",
    "    \n",
    "    # Ensure proper dimensionality\n",
    "    if len(state) < state_dim:\n",
    "        state = np.pad(state, (0, state_dim - len(state)))\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cd34d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a755e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dynamics_model(mpc_agent, env, config, args):\n",
    "    \"\"\"\n",
    "    Train MPC dynamics model through environment interaction.\n",
    "    \n",
    "    Phase 1: Random exploration to collect initial data\n",
    "    Phase 2: MPC-guided exploration with dynamics learning\n",
    "    Phase 3: Evaluation and comparison\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"MPC Dynamics Training\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    solve_times = []\n",
    "    dynamics_losses = []\n",
    "\n",
    "    current_episode_reward = np.zeros(mpc_agent.num_agents)\n",
    "    current_episode_length = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    agents = relocate_agents(env)\n",
    "    num_agents = len(agents)\n",
    "\n",
    "    cam_shape = env.observation_space(agents[0])[1].shape\n",
    "    vec_dim = (env.observation_space(agents[0])[0].shape[0] + \n",
    "            env.observation_space(agents[0])[2].shape[0])\n",
    "    vec_shape = (vec_dim,)\n",
    "    action_shape = env.action_space(agents[0]).shape\n",
    "\n",
    "    blank_cam = np.zeros(cam_shape, dtype=np.float32)\n",
    "    blank_vec = np.zeros(vec_shape, dtype=np.float32)\n",
    "\n",
    "    phases = [\n",
    "        (\"Random Exploration\", 0, args.exploration_steps),\n",
    "        (\"MPC Training\", args.exploration_steps, args.exploration_steps + args.training_steps),\n",
    "        (\"MPC Refinement\", args.exploration_steps + args.training_steps, args.max_steps)\n",
    "    ]\n",
    "\n",
    "    total_steps = 0\n",
    "\n",
    "    for phase_name, start_step, end_step in phases:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Phase: {phase_name}\")\n",
    "        print(f\"Steps: {start_step:,} - {end_step:,}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "\n",
    "        while total_steps < end_step:\n",
    "            if not obs or len(obs) == 0:\n",
    "                obs = env.reset()\n",
    "                agents = relocate_agents(env)\n",
    "                current_episode_reward = np.zeros(num_agents)\n",
    "                current_episode_length = 0\n",
    "\n",
    "            camera_obs = np.zeros((num_agents, *cam_shape), dtype=np.float32)\n",
    "            vector_obs = np.zeros((num_agents, *vec_shape), dtype=np.float32)\n",
    "            states = np.zeros((num_agents, config.state_dim), dtype=np.float32)\n",
    "\n",
    "            for i, agent_id in enumerate(agents):\n",
    "                if agent_id in obs:\n",
    "                    cam, vec = get_agent_obs(obs, agent_id)\n",
    "                else:\n",
    "                    cam, vec = blank_cam, blank_vec\n",
    "                camera_obs[i] = cam\n",
    "                vector_obs[i] = vec\n",
    "                states[i] = extract_state_from_obs(vec, config.state_dim)\n",
    "\n",
    "            if phase_name == \"Random Exploration\":\n",
    "                actions = np.random.uniform(-1, 1, (num_agents, config.action_dim))\n",
    "            else:\n",
    "                goals = generate_goals(vector_obs)\n",
    "                # Note: We pass full vector_obs, agent extracts kinematics internally\n",
    "                actions, info = mpc_agent.get_actions(vector_obs, goals)\n",
    "\n",
    "            action_dict = {agent_id: action for agent_id, action in zip(agents, actions)}\n",
    "            next_obs, reward_dict, done_dict, info_dict = env.step(action_dict)\n",
    "\n",
    "            next_states = np.zeros((num_agents, config.state_dim), dtype=np.float32)\n",
    "\n",
    "            for i, agent_id in enumerate(agents):\n",
    "                if agent_id in next_obs:\n",
    "                    _, next_vec = get_agent_obs(next_obs, agent_id)\n",
    "                    next_states[i] = extract_state_from_obs(next_vec, config.state_dim)\n",
    "            \n",
    "            transitions = []\n",
    "            transition_dict = {} \n",
    "            for i in range(num_agents):\n",
    "                transition_dict[i] = {\n",
    "                    'state': vector_obs[i],      # Pass full obs\n",
    "                    'action': actions[i],\n",
    "                    'next_state': next_vec[i] # Need to get next_vec from step\n",
    "                }\n",
    "            transitions.append(transition_dict)\n",
    "\n",
    "            if total_steps % args.dynamics_update_freq == 0 and total_steps > 0:\n",
    "                update_stats = mpc_agent.update_dynamics_models(transitions)\n",
    "                if update_stats:\n",
    "                    avg_loss = np.mean([s['dynamics_loss'] for s in update_stats])\n",
    "                    dynamics_losses.append(avg_loss)\n",
    "            else:\n",
    "                # Just store transitions\n",
    "                mpc_agent.update_dynamics_models(transitions)\n",
    "\n",
    "            rewards = np.array([reward_dict.get(a, 0.0) for a in agents])\n",
    "            dones = np.array([done_dict.get(a, False) for a in agents])\n",
    "            \n",
    "            current_episode_reward += rewards\n",
    "            current_episode_length += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            if any(dones) or all(done_dict.values()):\n",
    "                mean_reward = current_episode_reward.mean()\n",
    "                episode_rewards.append(mean_reward)\n",
    "                episode_lengths.append(current_episode_length)\n",
    "\n",
    "                obs = env.reset()\n",
    "                agents = relocate_agents(env)\n",
    "                current_episode_reward = np.zeros(num_agents)\n",
    "                current_episode_length = 0\n",
    "            else:\n",
    "                obs = next_obs\n",
    "\n",
    "            if total_steps % args.log_every == 0:\n",
    "                if len(episode_rewards) > 0:\n",
    "                    mean_reward = np.mean(episode_rewards[-100:])\n",
    "                    mean_length = np.mean(episode_lengths[-100:])\n",
    "                else:\n",
    "                    mean_reward = 0.0\n",
    "                    mean_length = 0.0\n",
    "\n",
    "            print(f\"\\nStep {total_steps:,} | Phase: {phase_name}\")\n",
    "            print(f\"  Reward (100ep): {mean_reward:.2f}\")\n",
    "            print(f\"  Episode length: {mean_length:.0f}\")\n",
    "            \n",
    "            if len(solve_times) > 0:\n",
    "                print(f\"  Avg solve time: {np.mean(solve_times[-100:])*1000:.1f} ms\")\n",
    "            \n",
    "            if len(dynamics_losses) > 0:\n",
    "                print(f\"  Dynamics loss:  {dynamics_losses[-1]:.4f}\")\n",
    "\n",
    "            if args.use_wandb:\n",
    "                log_dict = {\n",
    "                    'train/reward_mean': mean_reward,\n",
    "                        'train/episode_length': mean_length,\n",
    "                        'train/phase': phases.index((phase_name, start_step, end_step)),\n",
    "                        'train/total_steps': total_steps,\n",
    "                }\n",
    "                \n",
    "                if len(solve_times) > 0:\n",
    "                    log_dict['mpc/solve_time_ms'] = np.mean(solve_times[-100:]) * 1000\n",
    "                \n",
    "                if len(dynamics_losses) > 0:\n",
    "                    log_dict['mpc/dynamics_loss'] = dynamics_losses[-1]\n",
    "                \n",
    "                wandb.log(log_dict, step=total_steps)\n",
    "\n",
    "    return episode_rewards, solve_times, dynamics_losses\n",
    "\n",
    "def generate_goals(vector_obs):\n",
    "    \"\"\"\n",
    "    Simple goal logic: Move 10m forward relative to current position.\n",
    "    \"\"\"\n",
    "    positions = vector_obs[:, :3]\n",
    "    goals = positions.copy()\n",
    "    goals[:, 0] += 10.0 # Move 10 units in X\n",
    "    return goals\n",
    "\n",
    "def evaluate_mpc(mpc_agent, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate trained MPC agent.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MPC Evaluation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    solve_times_all = []\n",
    "    successes = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        agents = relocate_agents(env)\n",
    "        num_agents = len(agents)\n",
    "        \n",
    "        # Get shapes\n",
    "        cam_shape = env.observation_space(agents[0])[1].shape\n",
    "        vec_shape = (env.observation_space(agents[0])[0].shape[0] + \n",
    "                     env.observation_space(agents[0])[2].shape[0],)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done and episode_length < 500:\n",
    "            # Collect observations\n",
    "            camera_obs = []\n",
    "            vector_obs = []\n",
    "            \n",
    "            for agent_id in agents:\n",
    "                if agent_id in obs:\n",
    "                    cam, vec = get_agent_obs(obs, agent_id)\n",
    "                else:\n",
    "                    cam = np.zeros(cam_shape, dtype=np.float32)\n",
    "                    vec = np.zeros(vec_shape, dtype=np.float32)\n",
    "                camera_obs.append(cam)\n",
    "                vector_obs.append(vec)\n",
    "            \n",
    "            camera_obs = np.array(camera_obs)\n",
    "            vector_obs = np.array(vector_obs)\n",
    "            \n",
    "            # Get MPC actions\n",
    "            states = np.array([extract_state_from_obs(v, mpc_agent.config.state_dim) \n",
    "                              for v in vector_obs])\n",
    "            goals = generate_goals(states, mpc_agent.config)\n",
    "            \n",
    "            actions, info = mpc_agent.get_action(\n",
    "                camera_obs,\n",
    "                vector_obs,\n",
    "                goals=goals\n",
    "            )\n",
    "            \n",
    "            solve_times_all.append(info['solve_time'])\n",
    "            \n",
    "            # Step environment\n",
    "            action_dict = {agent_id: action for agent_id, action in zip(agents, actions)}\n",
    "            next_obs, reward_dict, done_dict, _ = env.step(action_dict)\n",
    "            \n",
    "            # Accumulate rewards\n",
    "            rewards = np.array([reward_dict.get(a, 0.0) for a in agents])\n",
    "            episode_reward += rewards.sum()\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Check done\n",
    "            done = all(done_dict.values())\n",
    "            obs = next_obs\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "        \n",
    "        if episode_reward > 40:  # Success threshold\n",
    "            successes += 1\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}: \"\n",
    "                  f\"Reward={episode_reward:.2f}, \"\n",
    "                  f\"Length={episode_length}, \"\n",
    "                  f\"Solve={np.mean(solve_times_all[-episode_length:])*1000:.1f}ms\")\n",
    "            \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Mean Reward:     {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Success Rate:    {successes/num_episodes:.1%}\")\n",
    "    print(f\"Mean Length:     {np.mean(episode_lengths):.1f}\")\n",
    "    print(f\"Avg Solve Time:  {np.mean(solve_times_all)*1000:.1f} ms\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'success_rate': successes / num_episodes,\n",
    "        'solve_times': solve_times_all,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae02d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self,d):\n",
    "        self.__dict__=d\n",
    "\n",
    "params = {\n",
    "    'env_path': 'Env/FinalLevel/DroneFlightv1',\n",
    "    'exploration_steps': 5000,\n",
    "    'training_steps': 20000,\n",
    "    'max_steps': 50000,\n",
    "    'log_every': 1000,\n",
    "    'dynamics_update_freq': 100,\n",
    "    'use_wandb': True\n",
    "}\n",
    "\n",
    "args = Args(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552d5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MPC Training for Multi-Agent Drone Navigation\n",
      "============================================================\n",
      "\n",
      "Loading Unity environment...\n",
      "✓ Environment loaded\n",
      "  Agents: 4\n",
      "  Camera shape: (4, 84, 84)\n",
      "  Vector dim: 92\n",
      "  Action dim: 4\n",
      "\n",
      "Initializing MPC...\n",
      "✓ MPC initialized\n",
      "  Horizon: 20\n",
      "  Dynamics LR: 0.001\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "MPC Dynamics Training\n",
      "============================================================\n",
      "\n",
      "========================================\n",
      "Phase: Random Exploration\n",
      "Steps: 0 - 5,000\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MPCConfig' object has no attribute 'state_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train dynamics model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m episode_rewards, solve_times, dynamics_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dynamics_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmpc_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[0;32m     52\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m Path(args\u001b[38;5;241m.\u001b[39msave_dir)\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mtrain_dynamics_model\u001b[1;34m(mpc_agent, env, config, args)\u001b[0m\n\u001b[0;32m     55\u001b[0m camera_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_agents, \u001b[38;5;241m*\u001b[39mcam_shape), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     56\u001b[0m vector_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_agents, \u001b[38;5;241m*\u001b[39mvec_shape), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 57\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_agents, \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dim\u001b[49m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agents):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m obs:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MPCConfig' object has no attribute 'state_dim'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MPC Training for Multi-Agent Drone Navigation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLoading Unity environment...\")\n",
    "env = UE(file_name=args.env_path)\n",
    "env = UPZBE(env)\n",
    "\n",
    "# Get environment info\n",
    "obs = env.reset()\n",
    "agents = relocate_agents(env)\n",
    "num_agents = len(agents)\n",
    "\n",
    "cam_shape = env.observation_space(agents[0])[1].shape\n",
    "vec_dim = (env.observation_space(agents[0])[0].shape[0] + \n",
    "            env.observation_space(agents[0])[2].shape[0])\n",
    "vec_shape = (vec_dim,)\n",
    "action_dim = env.action_space(agents[0]).shape[0]\n",
    "\n",
    "print(f\"✓ Environment loaded\")\n",
    "print(f\"  Agents: {num_agents}\")\n",
    "print(f\"  Camera shape: {cam_shape}\")\n",
    "print(f\"  Vector dim: {vec_dim}\")\n",
    "print(f\"  Action dim: {action_dim}\")\n",
    "\n",
    "# Create MPC configuration\n",
    "config = MPCConfig(\n",
    "    horizon=20,\n",
    "    dynamics_lr=1e-3,\n",
    "    action_dim=action_dim,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize MPC\n",
    "print(\"\\nInitializing MPC...\")\n",
    "mpc_agent = MultiAgentMPC(\n",
    "    num_agents=num_agents,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"✓ MPC initialized\")\n",
    "print(f\"  Horizon: {config.horizon}\")\n",
    "print(f\"  Dynamics LR: {config.dynamics_lr}\")\n",
    "\n",
    "# Train dynamics model\n",
    "print(\"\\nStarting training...\")\n",
    "episode_rewards, solve_times, dynamics_losses = train_dynamics_model(\n",
    "    mpc_agent, env, config, args\n",
    ")\n",
    "\n",
    "# Save models\n",
    "save_dir = Path(args.save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = save_dir / \"mpc_final.pth\"\n",
    "mpc_agent.save(save_path)\n",
    "print(f\"\\n✓ MPC models saved to {save_path}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating MPC...\")\n",
    "eval_results = evaluate_mpc(mpc_agent, env, args.eval_episodes)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "\n",
    "if args.use_wandb:\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
