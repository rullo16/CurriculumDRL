{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2701d5",
   "metadata": {},
   "source": [
    "# MAPPO Training for Multi-Agent Drone Navigation\n",
    "\n",
    "This notebook trains a MAPPO agent on your Unity ML-Agents drone environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c65b2",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ad1790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n",
      "GPU Memory: 17.17 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "# Unity ML-Agents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "\n",
    "# MAPPO components\n",
    "from MAPPO.mappo_agent import MAPPOAgent\n",
    "from MAPPO.rollout_buffer import RolloutBuffer\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1547475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Unity warnings suppressed\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Suppress Warnings\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings('ignore', module='mlagents_envs')\n",
    "logging.getLogger('mlagents_envs').setLevel(logging.ERROR)\n",
    "print(\"✓ Unity warnings suppressed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0a92",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e06922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Max steps: 3,000,000\n",
      "  Rollout length: 2048\n",
      "  Learning rate: 0.0003\n",
      "  calcuEntropy coefficient: 0.01\n",
      "  Curiosity coefficient: 0.1\n",
      "  Save directory: saved_models_mappo\n"
     ]
    }
   ],
   "source": [
    "# MAPPO Hyperparameters\n",
    "config = {\n",
    "    # Learning\n",
    "    'learning_rate': 3e-4,           # Slightly higher for faster adaptation\n",
    "    'clip_param': 0.2,\n",
    "    'value_loss_coef': 0.5,\n",
    "    'entropy_coef': 0.01,            # ← CHANGED: Higher for more exploration\n",
    "    'curiosity_coef': 0.1,          # ← CHANGED: Higher for curiosity\n",
    "    'max_grad_norm': 5.0,\n",
    "    \n",
    "    # GAE\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    \n",
    "    # Training\n",
    "    'rollout_length': 2048,\n",
    "    'num_minibatches': 8,\n",
    "    'ppo_epochs': 3,\n",
    "    'max_steps': 3_000_000,\n",
    "    'reward_clip': 10.0,\n",
    "}\n",
    "\n",
    "# Training settings\n",
    "SAVE_DIR = Path(\"./saved_models_mappo\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_EVERY = 1          # Log every N updates\n",
    "SAVE_EVERY = 10        # Save checkpoint every N updates\n",
    "\n",
    "USE_WANDB = True       # Enable Weights & Biases logging\n",
    "NORMALIZE_REWARDS = False  # Enable reward normalization (optional)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Max steps: {config['max_steps']:,}\")\n",
    "print(f\"  Rollout length: {config['rollout_length']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  calcuEntropy coefficient: {config['entropy_coef']}\")\n",
    "print(f\"  Curiosity coefficient: {config['curiosity_coef']}\")\n",
    "print(f\"  Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4856bc8",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16dd3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions loaded\n"
     ]
    }
   ],
   "source": [
    "def get_agent_obs(obs, agent, cam_key=1, vec_keys=[0, 2]):\n",
    "    \"\"\"\n",
    "    Extract observation data for an agent.\n",
    "    Returns camera (CHW, float32, [0,1]) and vector (1D, float32)\n",
    "    \"\"\"\n",
    "    if agent not in obs:\n",
    "        raise KeyError(f\"Agent {agent!r} not found in observations\")\n",
    "    \n",
    "    data = obs[agent]\n",
    "    if isinstance(data, dict) and \"observation\" in data:\n",
    "        data = data[\"observation\"]\n",
    "    \n",
    "    # Extract camera and vector observations\n",
    "    if isinstance(data, dict) and (\"camera_obs\" in data and \"vector_obs\" in data):\n",
    "        cam = np.asarray(data[\"camera_obs\"])\n",
    "        vec = np.asarray(data[\"vector_obs\"])\n",
    "        if vec.ndim > 1:\n",
    "            vec = vec.reshape(-1)\n",
    "    else:\n",
    "        # Indexed access\n",
    "        cam = np.asarray(data[cam_key])\n",
    "        v0 = np.asarray(data[vec_keys[0]]).reshape(-1)\n",
    "        v1 = np.asarray(data[vec_keys[1]]).reshape(-1)\n",
    "        vec = np.concatenate([v0, v1], axis=0)\n",
    "    \n",
    "    # Convert camera to CHW format and normalize to [0, 1]\n",
    "    if cam.ndim != 3:\n",
    "        raise AssertionError(f\"Camera must be 3D, got {cam.shape}\")\n",
    "    \n",
    "    if cam.shape[-1] in (1, 3, 4):  # HWC format\n",
    "        cam = np.transpose(cam, (2, 0, 1))  # Convert to CHW\n",
    "    \n",
    "    cam = cam.astype(np.float32, copy=False)\n",
    "    if cam.max() > 1.5:  # Likely uint8 [0..255]\n",
    "        cam = cam / 255.0\n",
    "    \n",
    "    vec = vec.astype(np.float32, copy=False)\n",
    "    \n",
    "    return cam, vec\n",
    "\n",
    "\n",
    "def relocate_agents(env):\n",
    "    \"\"\"Get sorted list of agent IDs\"\"\"\n",
    "    return sorted(list(env.agents))\n",
    "\n",
    "\n",
    "class RunningMeanStd:\n",
    "    \"\"\"Track running mean and std for reward normalization\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4\n",
    "    \n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x)\n",
    "        batch_var = np.var(x)\n",
    "        batch_count = len(x) if hasattr(x, '__len__') else 1\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "        \n",
    "        self.mean += delta * batch_count / tot_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        m2 = m_a + m_b + delta**2 * self.count * batch_count / tot_count\n",
    "        self.var = m2 / tot_count\n",
    "        self.count = tot_count\n",
    "    \n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0822cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageCurriculum:\n",
    "    def __init__(self, simple_env_path, complex_env_path,\n",
    "                 stage1_target_steps=500_000, stage1_success_threshold=0.70):\n",
    "        self.simple_env_path = simple_env_path\n",
    "        self.complex_env_path = complex_env_path\n",
    "        self.stage1_target_steps = stage1_target_steps\n",
    "        self.stage1_success_threshold = stage1_success_threshold\n",
    "        self.current_stage = 1\n",
    "        self.stage1_completed = False\n",
    "    \n",
    "    def should_advance_stage(self, current_steps, recent_success_rate):\n",
    "        if self.stage1_completed:\n",
    "            return False\n",
    "        \n",
    "        should_advance = (recent_success_rate >= self.stage1_success_threshold or \n",
    "                         current_steps >= self.stage1_target_steps)\n",
    "        \n",
    "        if should_advance:\n",
    "            print(f\"\\n{'='*70}\\nSTAGE 1 COMPLETE → STAGE 2\\n{'='*70}\")\n",
    "            self.stage1_completed = True\n",
    "            self.current_stage = 2\n",
    "        \n",
    "        return should_advance\n",
    "    \n",
    "    def get_current_env_path(self):\n",
    "        return self.complex_env_path if self.stage1_completed else self.simple_env_path\n",
    "\n",
    "# Initialize\n",
    "curriculum = TwoStageCurriculum(\n",
    "    simple_env_path='./Env/Level1/DroneFlightv1',\n",
    "    complex_env_path='./Env/FinalLevel/DroneFlightv1',\n",
    "    stage1_target_steps=500_000,\n",
    "    stage1_success_threshold=0.70\n",
    ")\n",
    "\n",
    "ENV_PATH = curriculum.get_current_env_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbea7d",
   "metadata": {},
   "source": [
    "## 4. Initialize Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322defbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Unity environment...\n",
      "\n",
      "✓ Environment initialized\n",
      "  Number of agents: 4\n",
      "  Camera shape: (4, 84, 84)\n",
      "  Vector dim: 92\n",
      "  Action dim: 4\n"
     ]
    }
   ],
   "source": [
    "# Load Unity environment\n",
    "NO_GRAPHICS = False  # Set to False to see visualization\n",
    "\n",
    "print(\"Loading Unity environment...\")\n",
    "env = UE(file_name=ENV_PATH, seed=SEED, no_graphics=NO_GRAPHICS)\n",
    "env = UPZBE(env)\n",
    "\n",
    "# Get environment info\n",
    "obs = env.reset()\n",
    "agents = relocate_agents(env)\n",
    "num_agents = len(agents)\n",
    "\n",
    "# Get observation and action spaces\n",
    "cam_shape = env.observation_space(agents[0])[1].shape\n",
    "vec_dim = (env.observation_space(agents[0])[0].shape[0] + \n",
    "           env.observation_space(agents[0])[2].shape[0])\n",
    "vec_shape = (vec_dim,)\n",
    "action_shape = env.action_space(agents[0]).shape\n",
    "\n",
    "print(\"\\n✓ Environment initialized\")\n",
    "print(f\"  Number of agents: {num_agents}\")\n",
    "print(f\"  Camera shape: {cam_shape}\")\n",
    "print(f\"  Vector dim: {vec_dim}\")\n",
    "print(f\"  Action dim: {action_shape[0]}\")\n",
    "\n",
    "# Create blank observations for missing agents\n",
    "blank_cam = np.zeros(cam_shape, dtype=np.float32)\n",
    "blank_vec = np.zeros(vec_shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51380b4",
   "metadata": {},
   "source": [
    "## 5. Initialize MAPPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3ebd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAPPO agent...\n",
      "\n",
      "✓ Loading pretrained features from SavedModels/feature_extractor_contrastive_init.pth\n",
      "✓ Pretrained features loaded successfully\n",
      "\n",
      "✓ Agent initialized\n",
      "  Actor parameters: 167,432\n",
      "  Critic parameters: 1,053,700\n",
      "  Vision encoder parameters: 389,664\n",
      "  Total trainable parameters: 1,610,796\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing MAPPO agent...\")\n",
    "\n",
    "agent = MAPPOAgent(\n",
    "    camera_shape=cam_shape,\n",
    "    vector_shape=vec_shape,\n",
    "    action_dim=action_shape[0],\n",
    "    num_agents=num_agents,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Load pretrained feature extractor if available\n",
    "PRETRAINED_PATH = \"SavedModels/feature_extractor_contrastive_init.pth\"\n",
    "if os.path.exists(PRETRAINED_PATH):\n",
    "    print(f\"\\n✓ Loading pretrained features from {PRETRAINED_PATH}\")\n",
    "    state_dict = torch.load(PRETRAINED_PATH, map_location=device)\n",
    "    agent.vision_encoder.load_state_dict(state_dict, strict=False)\n",
    "    print(\"✓ Pretrained features loaded successfully\")\n",
    "else:\n",
    "    print(f\"\\n Pretrained features not found at {PRETRAINED_PATH}\")\n",
    "    print(\"   Training from scratch (will take longer)\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n✓ Agent initialized\")\n",
    "print(f\"  Actor parameters: {count_parameters(agent.actor):,}\")\n",
    "print(f\"  Critic parameters: {count_parameters(agent.critic):,}\")\n",
    "print(f\"  Vision encoder parameters: {count_parameters(agent.vision_encoder):,}\")\n",
    "print(f\"  Total trainable parameters: {count_parameters(agent.actor) + count_parameters(agent.critic) + count_parameters(agent.vision_encoder):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638a2fa",
   "metadata": {},
   "source": [
    "## 5.1 Resume Training (Optional)\n",
    "\n",
    "If you want to resume training from a checkpoint, set `RESUME_TRAINING = True` and specify the checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a73d8045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Starting fresh training (no checkpoint loaded)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== RESUME TRAINING CONFIGURATION ====================\n",
    "RESUME_TRAINING = False  # Set to True to resume from checkpoint\n",
    "CHECKPOINT_PATH = \"saved_models_mappo/mappo_final.pth\"  # Update this path\n",
    "# ========================================================================\n",
    "\n",
    "if RESUME_TRAINING:\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"  RESUMING FROM CHECKPOINT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Loading checkpoint: {CHECKPOINT_PATH}\")\n",
    "        \n",
    "        agent.load(CHECKPOINT_PATH)\n",
    "        \n",
    "        print(\"✓ Checkpoint loaded successfully\")\n",
    "        print(\"\\nNote: You may want to adjust the following in the training loop:\")\n",
    "        print(\"  - total_steps (to continue from where you left off)\")\n",
    "        print(\"  - num_updates (checkpoint_steps / rollout_length)\")\n",
    "        print(\"  - best_reward (to your best known reward)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  WARNING: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "        print(\"   Starting training from scratch\\n\")\n",
    "        RESUME_TRAINING = False\n",
    "else:\n",
    "    print(\"\\n✓ Starting fresh training (no checkpoint loaded)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81686a6c",
   "metadata": {},
   "source": [
    "## 6. Initialize Rollout Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f914b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing rollout buffer...\n",
      "✓ Buffer created (capacity: 2048 steps)\n",
      "  Memory per rollout: ~12.6 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing rollout buffer...\")\n",
    "\n",
    "buffer = RolloutBuffer(\n",
    "    num_steps=config['rollout_length'],\n",
    "    num_agents=num_agents,\n",
    "    obs_shape=(agent.encoded_obs_dim,),\n",
    "    action_dim=action_shape[0],\n",
    "    gamma=config['gamma'],\n",
    "    gae_lambda=config['gae_lambda']\n",
    ")\n",
    "\n",
    "print(f\"✓ Buffer created (capacity: {config['rollout_length']} steps)\")\n",
    "print(f\"  Memory per rollout: ~{(config['rollout_length'] * num_agents * agent.encoded_obs_dim * 4) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298525e",
   "metadata": {},
   "source": [
    "## 7. Initialize Logging (Weights & Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3231a44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrullofederico16\u001b[0m (\u001b[33mfede-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Fede\\Desktop\\MasterThesis\\wandb\\run-20251113_105422-eayq9gt4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fede-/MAPPO_Drones/runs/eayq9gt4' target=\"_blank\">mappo_20251113_105421</a></strong> to <a href='https://wandb.ai/fede-/MAPPO_Drones' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fede-/MAPPO_Drones' target=\"_blank\">https://wandb.ai/fede-/MAPPO_Drones</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fede-/MAPPO_Drones/runs/eayq9gt4' target=\"_blank\">https://wandb.ai/fede-/MAPPO_Drones/runs/eayq9gt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ W&B initialized: mappo_20251113_105421\n",
      "  View at: https://wandb.ai/fede-/MAPPO_Drones/runs/eayq9gt4\n"
     ]
    }
   ],
   "source": [
    "if USE_WANDB:\n",
    "    run_name = f\"mappo_{dt.datetime.now():%Y%m%d_%H%M%S}\"\n",
    "    wandb.init(\n",
    "        project=os.getenv(\"WANDB_PROJECT\", \"MAPPO_Drones\"),\n",
    "        entity=os.getenv(\"WANDB_ENTITY\", \"fede-\"),\n",
    "        name=run_name,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"✓ W&B initialized: {run_name}\")\n",
    "    print(f\"  View at: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/runs/{wandb.run.id}\")\n",
    "else:\n",
    "    print(\"W&B logging disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd7f2c",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "This cell runs the main training loop:\n",
    "1. **Collection Phase**: Collect `rollout_length` steps (2048)\n",
    "2. **Advantage Computation**: Calculate advantages using GAE\n",
    "3. **Update Phase**: Train policy for `ppo_epochs` (4) epochs\n",
    "4. **Logging**: Track metrics and save checkpoints\n",
    "\n",
    "**Note:** This will run for ~24 hours if training to 3M steps. You can:\n",
    "- Stop anytime with Interrupt Kernel\n",
    "- Resume later using saved checkpoints\n",
    "- Reduce `config['max_steps']` for shorter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f196bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Target steps: 3,000,000\n",
      "Rollout length: 2048\n",
      "Update every: 2048 steps\n",
      "PPO epochs per update: 3\n",
      "\n",
      "Expected updates: 1,464\n",
      "Estimated time: ~24.0 hours\n",
      "\n",
      "Press 'Interrupt Kernel' to stop training at any time.\n",
      "Training will save checkpoints every 10 updates.\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Check success (adjust threshold based on your task)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m success \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(current_episode_reward[a] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m agents)\n\u001b[1;32m--> 153\u001b[0m episode_successes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Reset\u001b[39;00m\n\u001b[0;32m    156\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'generator'"
     ]
    }
   ],
   "source": [
    "# Training state\n",
    "total_steps = 0\n",
    "num_updates = 0\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "episode_successes = []\n",
    "\n",
    "# Current episode tracking\n",
    "current_episode_reward = np.zeros(num_agents)\n",
    "current_episode_length = 0\n",
    "\n",
    "# Reward normalization (optional)\n",
    "reward_normalizer = RunningMeanStd() if NORMALIZE_REWARDS else None\n",
    "\n",
    "# Best model tracking\n",
    "best_reward = -float('inf')\n",
    "success_rate = 0.0\n",
    "\n",
    "# Reset environment\n",
    "obs = env.reset()\n",
    "agents = relocate_agents(env)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target steps: {config['max_steps']:,}\")\n",
    "print(f\"Rollout length: {config['rollout_length']}\")\n",
    "print(f\"Update every: {config['rollout_length']} steps\")\n",
    "print(f\"PPO epochs per update: {config['ppo_epochs']}\")\n",
    "print(f\"\\nExpected updates: {config['max_steps'] // config['rollout_length']:,}\")\n",
    "print(f\"Estimated time: ~{config['max_steps'] / 125_000:.1f} hours\")\n",
    "print(\"\\nPress 'Interrupt Kernel' to stop training at any time.\")\n",
    "print(f\"Training will save checkpoints every {SAVE_EVERY} updates.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    while total_steps < config['max_steps']:\n",
    "        \n",
    "        # Check for stage transition\n",
    "        if curriculum.should_advance_stage(total_steps, success_rate):\n",
    "            env.close()\n",
    "            ENV_PATH = curriculum.get_current_env_path()\n",
    "            env = UE(file_name=ENV_PATH, seed=SEED, no_graphics=NO_GRAPHICS)\n",
    "            env = UPZBE(env)\n",
    "            obs = env.reset()\n",
    "            agents = relocate_agents(env)\n",
    "            print(f\"✓ Loaded: {ENV_PATH}\\n\")\n",
    "\n",
    "            CHECKPOINT_PATH = SAVE_DIR / \"mappo_stage1_checkpoint.pth\"\n",
    "            agent.save(CHECKPOINT_PATH)\n",
    "            print(f\"✓ Stage 1 model saved: {CHECKPOINT_PATH}\\n\")\n",
    "\n",
    "        # ================================================================\n",
    "        # COLLECTION PHASE: Gather trajectories\n",
    "        # ================================================================\n",
    "        \n",
    "        for step in range(config['rollout_length']):\n",
    "            # Check if environment needs reset\n",
    "            if not obs or len(obs) == 0:\n",
    "                obs = env.reset()\n",
    "                agents = relocate_agents(env)\n",
    "                current_episode_reward = np.zeros(num_agents)\n",
    "                current_episode_length = 0\n",
    "            \n",
    "            # Get live agents\n",
    "            live_agents = relocate_agents(env)\n",
    "            \n",
    "            # Collect observations for all agents\n",
    "            camera_obs = np.zeros((num_agents, *cam_shape), dtype=np.float32)\n",
    "            vector_obs = np.zeros((num_agents, *vec_shape), dtype=np.float32)\n",
    "            \n",
    "            for i, agent_id in enumerate(agents):\n",
    "                if agent_id in obs:\n",
    "                    cam, vec = get_agent_obs(obs, agent_id)\n",
    "                else:\n",
    "                    cam, vec = blank_cam, blank_vec\n",
    "                camera_obs[i] = cam\n",
    "                vector_obs[i] = vec\n",
    "            \n",
    "            # Encode observations\n",
    "            encoded_obs = agent.encode_observations(camera_obs, vector_obs)\n",
    "            \n",
    "            # Get actions from policy\n",
    "            actions, log_probs, values = agent.get_action(\n",
    "                camera_obs,\n",
    "                vector_obs,\n",
    "                deterministic=False\n",
    "            )\n",
    "            \n",
    "            # Step environment\n",
    "            action_dict = {agent_id: action for agent_id, action in zip(agents, actions)}\n",
    "            next_obs, reward_dict, done_dict, info_dict = env.step(action_dict)\n",
    "            \n",
    "            # Collect rewards and dones\n",
    "            rewards = np.array([reward_dict.get(a, 0.0) for a in agents])\n",
    "            dones = np.array([done_dict.get(a, False) for a in agents], dtype=np.float32)\n",
    "\n",
    "\n",
    "            # Clip rewards\n",
    "            rewards = np.clip(rewards, -config['reward_clip'], config['reward_clip'])\n",
    "\n",
    "            #Compute intrinsic curiosity rewards\n",
    "            camera_obs_next = np.zeros((num_agents, *cam_shape), dtype=np.float32)\n",
    "            vector_obs_next = np.zeros((num_agents, *vec_shape), dtype=np.float32)\n",
    "\n",
    "            for i, agent_id in enumerate(agents):\n",
    "                if agent_id in next_obs:\n",
    "                    cam_next, vec_next = get_agent_obs(next_obs, agent_id)\n",
    "                else:\n",
    "                    cam_next, vec_next = blank_cam, blank_vec\n",
    "                camera_obs_next[i] = cam_next\n",
    "                vector_obs_next[i] = vec_next\n",
    "\n",
    "            encoded_obs = agent.encode_observations(camera_obs, vector_obs)\n",
    "            encoded_obs_next = agent.encode_observations(camera_obs_next, vector_obs_next)\n",
    "\n",
    "            intrinsic_rewards = agent.compute_intrinsic_rewards(\n",
    "                encoded_obs,\n",
    "                encoded_obs_next,\n",
    "                torch.from_numpy(actions).float().to(device)\n",
    "            )\n",
    "\n",
    "            total_rewards = rewards + intrinsic_rewards * config['curiosity_coef']\n",
    "            \n",
    "            # Normalize rewards (optional)\n",
    "            if NORMALIZE_REWARDS:\n",
    "                reward_normalizer.update(total_rewards)\n",
    "                total_rewards = (total_rewards - reward_normalizer.mean) / (reward_normalizer.std + 1e-8)\n",
    "            \n",
    "            # Store transition in buffer\n",
    "            buffer.store(\n",
    "                obs=encoded_obs.detach().cpu().numpy(),\n",
    "                action=actions,\n",
    "                reward=total_rewards,\n",
    "                done=dones,\n",
    "                value=values,\n",
    "                log_prob=log_probs\n",
    "            )\n",
    "            \n",
    "            # Update episode statistics\n",
    "            current_episode_reward += rewards\n",
    "            current_episode_length += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Check for episode end\n",
    "            if any(dones) or all(done_dict.values()):\n",
    "                mean_reward = current_episode_reward.mean()\n",
    "                episode_rewards.append(mean_reward)\n",
    "                episode_lengths.append(current_episode_length)\n",
    "                \n",
    "                # Check success (adjust threshold based on your task)\n",
    "                success = np.any(current_episode_reward > 15)\n",
    "                episode_successes.append(float(success))\n",
    "                \n",
    "                # Reset\n",
    "                obs = env.reset()\n",
    "                agents = relocate_agents(env)\n",
    "                current_episode_reward = np.zeros(num_agents)\n",
    "                current_episode_length = 0\n",
    "            else:\n",
    "                obs = next_obs\n",
    "        \n",
    "        # ================================================================\n",
    "        # UPDATE PHASE: Train policy with collected data\n",
    "        # ================================================================\n",
    "        \n",
    "        # Get final value estimates for GAE\n",
    "        camera_obs_final = np.zeros((num_agents, *cam_shape), dtype=np.float32)\n",
    "        vector_obs_final = np.zeros((num_agents, *vec_shape), dtype=np.float32)\n",
    "        \n",
    "        for i, agent_id in enumerate(agents):\n",
    "            if agent_id in obs:\n",
    "                cam, vec = get_agent_obs(obs, agent_id)\n",
    "            else:\n",
    "                cam, vec = blank_cam, blank_vec\n",
    "            camera_obs_final[i] = cam\n",
    "            vector_obs_final[i] = vec\n",
    "        \n",
    "        _, _, last_values = agent.get_action(camera_obs_final, vector_obs_final)\n",
    "        \n",
    "        # Compute returns and advantages using GAE\n",
    "        buffer.compute_returns_and_advantages(last_values)\n",
    "        \n",
    "        # Update policy\n",
    "        train_stats = agent.train(buffer)\n",
    "        num_updates += 1\n",
    "        \n",
    "        # ================================================================\n",
    "        # LOGGING PHASE: Record metrics\n",
    "        # ================================================================\n",
    "        \n",
    "        if num_updates % LOG_EVERY == 0:\n",
    "            # Compute statistics\n",
    "            mean_reward = np.mean(episode_rewards[-100:]) if episode_rewards else 0.0\n",
    "            mean_length = np.mean(episode_lengths[-100:]) if episode_lengths else 0.0\n",
    "            success_rate = np.mean(episode_successes[-100:]) if episode_successes else 0.0\n",
    "            \n",
    "            # Console logging\n",
    "            print(f\"\\nStep {total_steps:,} | Update {num_updates}\")\n",
    "            print(f\"  Reward (100ep):   {mean_reward:8.2f}\")\n",
    "            print(f\"  Success rate:     {success_rate:8.1%}\")\n",
    "            print(f\"  Episode length:   {mean_length:8.1f}\")\n",
    "            print(f\"  Policy loss:      {train_stats['policy_loss']:8.4f}\")\n",
    "            print(f\"  Value loss:       {train_stats['value_loss']:8.4f}\")\n",
    "            print(f\"  Entropy:          {train_stats['entropy']:8.4f}\")\n",
    "            print(f\"  KL divergence:    {train_stats['approx_kl']:8.4f}\")\n",
    "            print(f\"  Clip fraction:    {train_stats['clip_fraction']:8.1%}\")\n",
    "            print(f\"  Explained var:    {train_stats['explained_variance']:8.1%}\")\n",
    "            \n",
    "            # W&B logging\n",
    "            if USE_WANDB:\n",
    "                wandb.log({\n",
    "                    'train/reward_mean': mean_reward,\n",
    "                    'train/success_rate': success_rate,\n",
    "                    'train/episode_length': mean_length,\n",
    "                    'train/policy_loss': train_stats['policy_loss'],\n",
    "                    'train/value_loss': train_stats['value_loss'],\n",
    "                    'train/entropy': train_stats['entropy'],\n",
    "                    'train/approx_kl': train_stats['approx_kl'],\n",
    "                    'train/clip_fraction': train_stats['clip_fraction'],\n",
    "                    'train/explained_variance': train_stats['explained_variance'],\n",
    "                    'train/total_steps': total_steps,\n",
    "                }, step=total_steps)\n",
    "        \n",
    "        # ================================================================\n",
    "        # CHECKPOINT PHASE: Save model\n",
    "        # ================================================================\n",
    "        \n",
    "        if num_updates % SAVE_EVERY == 0:\n",
    "            save_path = SAVE_DIR / f\"mappo_checkpoint_{total_steps:08d}.pth\"\n",
    "            agent.save(save_path)\n",
    "            print(f\"  ✓ Checkpoint saved: {save_path.name}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if episode_rewards and mean_reward > best_reward:\n",
    "            best_reward = mean_reward\n",
    "            save_path = SAVE_DIR / \"mappo_best.pth\"\n",
    "            agent.save(save_path)\n",
    "            print(f\"  ✓ New best model saved: {mean_reward:.2f}\")\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  TRAINING INTERRUPTED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Completed {total_steps:,} steps ({num_updates} updates)\")\n",
    "    print(\"Saving final checkpoint...\")\n",
    "\n",
    "# Save final model\n",
    "final_path = SAVE_DIR / \"mappo_final.pth\"\n",
    "agent.save(final_path)\n",
    "print(f\"\\n✓ Final model saved: {final_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total steps: {total_steps:,}\")\n",
    "print(f\"Total updates: {num_updates}\")\n",
    "if episode_rewards:\n",
    "    print(f\"Final reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "if episode_successes:\n",
    "    print(f\"Final success rate: {np.mean(episode_successes[-100:]):.1%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6daba03",
   "metadata": {},
   "source": [
    "## 9. Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "print(\"✓ Environment closed\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()\n",
    "    print(\"✓ W&B run finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f36578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
