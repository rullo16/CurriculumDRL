{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from PPO_Distillation.DistilledPPOAgent import DistilledPPO\n",
    "from PPO_Distillation.Trajectories import ExperienceBuffer\n",
    "from PPO_Distillation.Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents():\n",
    "    return [agent for agent in env.agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=9']\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents()\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  ...\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]\n",
      "  [0.8039215 0.8039215 0.8039215 ... 0.8039215 0.8039215 0.8039215]]]\n",
      "Possible actions: [ 0  0 -1 -1 -1  0]\n",
      "(6,)\n",
      "(1, 84, 84)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs[agents[0]][1])\n",
    "possible_actions = env.action_space(agents[0]).sample()\n",
    "print(f\"Possible actions: {possible_actions}\")\n",
    "print(env.action_space(agents[0]).shape)\n",
    "print(env.observation_space(agents[0])[1].shape)\n",
    "print(env.observation_space(agents[0])[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape,env.action_space(agents[0]).shape, params['ppo_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1\n",
      "Finished episode 2\n",
      "Finished episode 3\n",
      "Finished episode 4\n",
      "Finished episode 5\n",
      "Finished Rnd Exploration\n"
     ]
    }
   ],
   "source": [
    "for s in range(1, params['ppo_distilled'].seed_episodes + 1):\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    while not all(done) or t < params['ppo_distilled'].n_steps_random_exploration:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents()\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            if isinstance(obs[agent], list):\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs[agent][1], obs[agent][2])\n",
    "            else:\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs[agent]['observation'][1], obs[agent]['observation'][2])\n",
    "            t+=1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            if isinstance(obs[agent], list):\n",
    "                Buffer.add(np.array(obs[agent][1]), np.array(obs[agent][2]), actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            else:\n",
    "                Buffer.add(np.array(obs[agent]['observation'][1]),np.array(obs[agent]['observation'][2]), actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "    print(f'Finished episode {s}')\n",
    "\n",
    "Buffer.compute_advantages_and_returns()\n",
    "print(\"Finished Rnd Exploration\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)\n",
    "agents = relocate_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents()\n",
    "brain.train(1, Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 18926\n",
      "Mean reward: -0.16706667840480804\n",
      "Finished episode 42926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 66926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 90926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 114926\n",
      "Mean reward: -0.12539999186992645\n",
      "Finished episode 138926\n",
      "Mean reward: -0.20873336493968964\n",
      "Finished episode 162926\n",
      "Mean reward: -0.16706667840480804\n",
      "Finished episode 186926\n",
      "Mean reward: -0.12540000677108765\n",
      "Finished episode 210926\n",
      "Mean reward: -0.41706669330596924\n",
      "Finished episode 234926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 258926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 282926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 306926\n",
      "Mean reward: -0.29206669330596924\n",
      "Finished episode 330926\n",
      "Mean reward: -0.16706670820713043\n",
      "Finished episode 354926\n",
      "Mean reward: -0.16706669330596924\n",
      "Finished episode 378926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 402926\n",
      "Mean reward: -0.16706669330596924\n",
      "Finished episode 426926\n",
      "Mean reward: -0.20873337984085083\n",
      "Finished episode 450926\n",
      "Mean reward: -0.08373332768678665\n",
      "Finished episode 474926\n",
      "Mean reward: -0.00039999993168748915\n",
      "Finished episode 498926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 522926\n",
      "Mean reward: -0.16706669330596924\n",
      "Finished episode 546926\n",
      "Mean reward: -0.29206669330596924\n",
      "Finished episode 570926\n",
      "Mean reward: -0.20873336493968964\n",
      "Finished episode 594926\n",
      "Mean reward: -0.3754000663757324\n",
      "Finished episode 618926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 642926\n",
      "Mean reward: -0.20873336493968964\n",
      "Finished episode 666926\n",
      "Mean reward: -0.16706669330596924\n",
      "Finished episode 690926\n",
      "Mean reward: -0.20873336493968964\n",
      "Finished episode 714926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 738926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 762926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 786926\n",
      "Mean reward: -0.41706669330596924\n",
      "Finished episode 810926\n",
      "Mean reward: -0.16706667840480804\n",
      "Finished episode 834926\n",
      "Mean reward: -0.20873336493968964\n",
      "Finished episode 858926\n",
      "Mean reward: -0.12539999186992645\n",
      "Finished episode 882926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 906926\n",
      "Mean reward: -0.25040003657341003\n",
      "Finished episode 930926\n",
      "Mean reward: -0.2920667231082916\n",
      "Finished episode 954926\n",
      "Mean reward: -0.16706669330596924\n",
      "Finished episode 978926\n",
      "Mean reward: -0.12539999186992645\n",
      "Finished episode 1002926\n",
      "Mean reward: -0.29206666350364685\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "while steps < params['ppo_distilled'].max_steps:\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    episode_reward = 0\n",
    "    while not all(done) or t < params['ppo_distilled'].n_steps:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents()\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            if isinstance(obs[agent], list):\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs[agent][1], obs[agent][2])\n",
    "            else:\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs[agent]['observation'][1], obs[agent]['observation'][2])\n",
    "            t += 1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            if isinstance(obs[agent], list):\n",
    "                Buffer.add(np.array(obs[agent][1]), np.array(obs[agent][2]), actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            else:\n",
    "                Buffer.add(np.array(obs[agent]['observation'][1]),np.array(obs[agent]['observation'][2]), actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "        tot_reward = [reward[agent] for agent in agents if agent in reward.keys()]\n",
    "    obs_keys = list(obs.keys())\n",
    "    _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "    Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "    mean_reward = np.mean(tot_reward)\n",
    "    \n",
    "    steps += t\n",
    "\n",
    "    brain.train(steps, Buffer)\n",
    "    Buffer.compute_advantages_and_returns()\n",
    "    brain.optimizer = brain.improv_lr(brain.optimizer, params['ppo_distilled'].lr,steps, params['ppo_distilled'].n_steps)\n",
    "    # brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['ppo_distilled'].lr,steps, params['ppo_distilled'].n_steps)\n",
    "    print(f'Finished episode {steps}')\n",
    "    print(f\"Mean reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(brain.net.state_dict(), \"PPO_distilled_checkpoint.pth\")\n",
    "print(\"Checkpoint saved successfully.\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ONNX format successfully.\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# brain.net.to(device)\n",
    "\n",
    "# # Ensure the model is in evaluation mode\n",
    "# brain.net.eval()\n",
    "\n",
    "# # Create dummy input matching the expected input format of the model\n",
    "# dummy_input_1 = torch.randn(1, *env.observation_space(agents[0])[1].shape).to(device)\n",
    "# dummy_input_2 = torch.randn(1, *env.observation_space(agents[0])[2].shape).to(device)\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# torch.onnx.export(\n",
    "#     brain.net,\n",
    "#     (dummy_input_1, dummy_input_2),\n",
    "#     \"PPO_distilled.onnx\",\n",
    "#     export_params=True,\n",
    "#     opset_version=10,\n",
    "#     do_constant_folding=True,\n",
    "#     input_names=[\"observation1\", \"observation2\"],\n",
    "#     output_names=[\"action\"],\n",
    "# )\n",
    "# print(\"Model exported to ONNX format successfully.\")\n",
    "\n",
    "# # Dispose of the dummy input tensors\n",
    "# del dummy_input_1\n",
    "# del dummy_input_2\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
