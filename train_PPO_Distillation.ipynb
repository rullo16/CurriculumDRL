{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv as UPZBE\n",
    "from PPO_Distillation.DistilledPPOAgent import DistilledPPO\n",
    "from PPO_Distillation.Trajectories import ExperienceBuffer\n",
    "from PPO_Distillation.Hyperparameters import HYPERPARAMS as params\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrullofederico16\u001b[0m (\u001b[33mfede-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rullo\\Desktop\\Master_Thesis\\Project\\Dreamer\\wandb\\run-20250220_122931-91wq0b8a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fede-/PPO_Distillation/runs/91wq0b8a' target=\"_blank\">polar-rain-30</a></strong> to <a href='https://wandb.ai/fede-/PPO_Distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fede-/PPO_Distillation' target=\"_blank\">https://wandb.ai/fede-/PPO_Distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fede-/PPO_Distillation/runs/91wq0b8a' target=\"_blank\">https://wandb.ai/fede-/PPO_Distillation/runs/91wq0b8a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"PPO_Distillation\", entity=\"fede-\")\n",
    "wandb.config.update(params['ppo_distilled'])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.config.update({\"device\": device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_agents(env):\n",
    "    return list(env.agents)  # simplified\n",
    "\n",
    "# New helper to extract observation data for an agent\n",
    "def get_agent_obs(obs, agent):\n",
    "    agent_data = obs[agent]\n",
    "    return np.array(agent_data[1]), np.array(agent_data[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999, step=0.001)\n",
    "    entropy_coef = trial.suggest_float('entropy_coef', 0.01, 0.1, step=0.01)\n",
    "    value_loss_coef = trial.suggest_float('value_loss_coef', 0.1, 1.0, step=0.1)\n",
    "    clip_grad_norm = trial.suggest_float('clip_grad_norm', 0.1, 1.0, step=0.1)\n",
    "    action_std = trial.suggest_float('action_std', 0.1, 1.0, step=0.1)\n",
    "\n",
    "    # Update the hyperparameters in the configuration\n",
    "    params['ppo_distilled'].lr = lr\n",
    "    params['ppo_distilled'].gamma = gamma\n",
    "    params['ppo_distilled'].entropy_coef = entropy_coef\n",
    "    params['ppo_distilled'].value_loss_coef = value_loss_coef\n",
    "    params['ppo_distilled'].clip_grad_norm = clip_grad_norm\n",
    "    params['ppo_distilled'].action_std = action_std\n",
    "\n",
    "    # Initialize the agent\n",
    "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "    env = UPZBE(env)\n",
    "    agents = relocate_agents(env)\n",
    "    brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "    Buffer = ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])\n",
    "\n",
    "    # Training loop\n",
    "    steps = 0\n",
    "    while steps < 100000:\n",
    "        obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "        while not all(done) or t < params['ppo_distilled'].n_steps:\n",
    "            actions, log_probs, values = {}, {}, {}\n",
    "            agents = relocate_agents(env)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "                t += 1\n",
    "\n",
    "            obs, reward, done, _ = env.step(actions)\n",
    "            for agent in agents:\n",
    "                if agent not in obs:\n",
    "                    continue\n",
    "                obs1, obs2 = get_agent_obs(obs, agent)\n",
    "                Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "            done = [done[agent] for agent in agents if agent in done]\n",
    "            tot_reward = [reward[agent] for agent in agents if agent in reward]\n",
    "        \n",
    "        obs_keys = list(obs.keys())\n",
    "        _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "        Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "        mean_reward = np.mean(tot_reward)\n",
    "        \n",
    "        steps += t\n",
    "\n",
    "        brain.train(steps, Buffer)\n",
    "        Buffer.compute_advantages_and_returns()\n",
    "        brain.optimizer = brain.improv_lr(brain.optimizer, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "        brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['ppo_distilled'].lr, steps, params['ppo_distilled'].n_steps)\n",
    "    \n",
    "    env.close()\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 23:57:09,305] A new study created in memory with name: no-name-9de0f2bb-bff9-4e4c-97df-c7a52bb97d32\n",
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "[I 2025-02-20 00:18:28,236] Trial 0 finished with value: -0.29206669330596924 and parameters: {'lr': 1.4235847482036186e-05, 'gamma': 0.909, 'entropy_coef': 0.09999999999999999, 'value_loss_coef': 1.0, 'clip_grad_norm': 0.8, 'action_std': 0.8}. Best is trial 0 with value: -0.29206669330596924.\n",
      "C:\\Users\\rullo\\AppData\\Local\\optunahub\\cache\\api.github.com\\optuna\\optunahub-registry\\main\\package\\samplers/auto_sampler\\_sampler.py:184: ExperimentalWarning: GPSampler is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  return GPSampler(seed=seed)\n",
      "[I 2025-02-20 00:39:56,797] Trial 1 finished with value: -0.29206669330596924 and parameters: {'lr': 0.0006193430342324773, 'gamma': 0.97, 'entropy_coef': 0.09999999999999999, 'value_loss_coef': 0.6, 'clip_grad_norm': 0.1, 'action_std': 0.9}. Best is trial 0 with value: -0.29206669330596924.\n",
      "[I 2025-02-20 01:01:53,822] Trial 2 finished with value: -0.16706667840480804 and parameters: {'lr': 2.0351082291802278e-05, 'gamma': 0.907, 'entropy_coef': 0.04, 'value_loss_coef': 0.2, 'clip_grad_norm': 0.30000000000000004, 'action_std': 0.30000000000000004}. Best is trial 2 with value: -0.16706667840480804.\n",
      "[I 2025-02-20 01:25:24,446] Trial 3 finished with value: -0.20873336493968964 and parameters: {'lr': 1.7940188666462062e-05, 'gamma': 0.9420000000000001, 'entropy_coef': 0.09, 'value_loss_coef': 0.6, 'clip_grad_norm': 0.5, 'action_std': 0.9}. Best is trial 2 with value: -0.16706667840480804.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Environment timed out shutting down. Killing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-02-20 08:51:30,161] Trial 4 failed with parameters: {'lr': 0.0007267685565859478, 'gamma': 0.9610000000000001, 'entropy_coef': 0.02, 'value_loss_coef': 0.8, 'clip_grad_norm': 0.9, 'action_std': 0.30000000000000004} because of the following error: UnityTimeOutException('The Unity environment took too long to respond. Make sure that :\\n\\t The environment does not need user interaction to launch\\n\\t The Agents\\' Behavior Parameters > Behavior Type is set to \"Default\"\\n\\t The environment and the Python interface have compatible versions.\\n\\t If you\\'re running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\rullo\\AppData\\Local\\Temp\\ipykernel_7720\\514747813.py\", line 21, in objective\n",
      "    env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py\", line 236, in __init__\n",
      "    aca_output = self._send_academy_parameters(rl_init_parameters_in)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py\", line 492, in _send_academy_parameters\n",
      "    return self._communicator.initialize(inputs, self._poll_process)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\", line 126, in initialize\n",
      "    self.poll_for_timeout(poll_callback)\n",
      "  File \"c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\", line 114, in poll_for_timeout\n",
      "    raise UnityTimeOutException(\n",
      "mlagents_envs.exception.UnityTimeOutException: The Unity environment took too long to respond. Make sure that :\n",
      "\t The environment does not need user interaction to launch\n",
      "\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n",
      "\t The environment and the Python interface have compatible versions.\n",
      "\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.\n",
      "[W 2025-02-20 08:51:30,175] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n\t The environment and the Python interface have compatible versions.\n\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m best_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mAutoSampler())\n\u001b[1;32m----> 6\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_distilled\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39maction_std \u001b[38;5;241m=\u001b[39m action_std\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize the agent\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mUE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDroneFlightv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_graphics_monitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_graphics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m env \u001b[38;5;241m=\u001b[39m UPZBE(env)\n\u001b[0;32m     23\u001b[0m agents \u001b[38;5;241m=\u001b[39m relocate_agents(env)\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py:236\u001b[0m, in \u001b[0;36mUnityEnvironment.__init__\u001b[1;34m(self, file_name, worker_id, base_port, seed, no_graphics, no_graphics_monitor, timeout_wait, additional_args, side_channels, log_folder, num_areas)\u001b[0m\n\u001b[0;32m    228\u001b[0m rl_init_parameters_in \u001b[38;5;241m=\u001b[39m UnityRLInitializationInputProto(\n\u001b[0;32m    229\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    230\u001b[0m     communication_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAPI_VERSION,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     num_areas\u001b[38;5;241m=\u001b[39mnum_areas,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     aca_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_academy_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrl_init_parameters_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     aca_params \u001b[38;5;241m=\u001b[39m aca_output\u001b[38;5;241m.\u001b[39mrl_initialization_output\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnityTimeOutException:\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\environment.py:492\u001b[0m, in \u001b[0;36mUnityEnvironment._send_academy_parameters\u001b[1;34m(self, init_parameters)\u001b[0m\n\u001b[0;32m    490\u001b[0m inputs \u001b[38;5;241m=\u001b[39m UnityInputProto()\n\u001b[0;32m    491\u001b[0m inputs\u001b[38;5;241m.\u001b[39mrl_initialization_input\u001b[38;5;241m.\u001b[39mCopyFrom(init_parameters)\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:126\u001b[0m, in \u001b[0;36mRpcCommunicator.initialize\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: UnityInputProto, poll_callback: Optional[PollCallback] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m UnityOutputProto:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll_for_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     aca_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munity_to_external\u001b[38;5;241m.\u001b[39mparent_conn\u001b[38;5;241m.\u001b[39mrecv()\u001b[38;5;241m.\u001b[39munity_output\n\u001b[0;32m    128\u001b[0m     message \u001b[38;5;241m=\u001b[39m UnityMessageProto()\n",
      "File \u001b[1;32mc:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py:114\u001b[0m, in \u001b[0;36mRpcCommunicator.poll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    111\u001b[0m         poll_callback()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Got this far without reading any data from the connection, so it must be dead.\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnityTimeOutException(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Unity environment took too long to respond. Make sure that :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The environment does not need user interaction to launch\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The Agents\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m Behavior Parameters > Behavior Type is set to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m The environment and the Python interface have compatible versions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre running on a headless server without graphics support, turn off display \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby either passing --no-graphics option or build your Unity executable as server build.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n",
      "\u001b[1;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n\t The environment and the Python interface have compatible versions.\n\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build."
     ]
    }
   ],
   "source": [
    "import optunahub\n",
    "\n",
    "module = optunahub.load_module(package='samplers/auto_sampler')\n",
    "best_params = {}\n",
    "study = optuna.create_study(direction='maximize', sampler=module.AutoSampler())\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UE(file_name=\"DroneFlightv1\", seed=1, side_channels=[], no_graphics_monitor=True, no_graphics=True)\n",
    "env = UPZBE(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone?team=0?agent_id=0', 'Drone?team=0?agent_id=1', 'Drone?team=0?agent_id=10', 'Drone?team=0?agent_id=11', 'Drone?team=0?agent_id=2', 'Drone?team=0?agent_id=3', 'Drone?team=0?agent_id=4', 'Drone?team=0?agent_id=5', 'Drone?team=0?agent_id=6', 'Drone?team=0?agent_id=7', 'Drone?team=0?agent_id=8', 'Drone?team=0?agent_id=9']\n"
     ]
    }
   ],
   "source": [
    "agents = relocate_agents(env)\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = env.reset()\n",
    "# print(obs[agents[0]][1])\n",
    "# possible_actions = env.action_space(agents[0]).sample()\n",
    "# print(f\"Possible actions: {possible_actions}\")\n",
    "# print(env.action_space(agents[0]).shape)\n",
    "# print(env.observation_space(agents[0])[1].shape)\n",
    "# print(env.observation_space(agents[0])[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = ExperienceBuffer(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape,env.action_space(agents[0]).shape, params['ppo_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1\n",
      "Finished episode 2\n",
      "Finished episode 3\n",
      "Finished episode 4\n",
      "Finished episode 5\n",
      "Finished Rnd Exploration\n"
     ]
    }
   ],
   "source": [
    "for s in range(1, params['ppo_distilled'].seed_episodes + 1):\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    while not all(done) or t < params['ppo_distilled'].n_steps_random_exploration:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t+=1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "    print(f'Finished episode {s}')\n",
    "\n",
    "Buffer.compute_advantages_and_returns()\n",
    "print(\"Finished Rnd Exploration\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = DistilledPPO(env.observation_space(agents[0])[1].shape, env.observation_space(agents[0])[2].shape, env.action_space(agents[0]).shape, params['ppo_distilled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rullo\\anaconda3\\envs\\ml_agents\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "brain.fine_tune_teacher(Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "best_mean_reward = -np.inf\n",
    "not_improved = 0\n",
    "while steps < params['ppo_distilled'].max_steps:\n",
    "    obs, done, t = env.reset(), [False for _ in env.agents], 0\n",
    "    episode_reward = 0\n",
    "    while not all(done) or t < params['ppo_distilled'].n_steps:\n",
    "        actions = {}\n",
    "        log_probs = {}\n",
    "        values = {}\n",
    "        agents = relocate_agents(env)\n",
    "        for agent in agents:\n",
    "            # actions[agent] = env.action_space(agent).sample()\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            actions[agent], log_probs[agent], values[agent] = brain.get_action(obs1, obs2)\n",
    "            t += 1\n",
    "\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        for agent in agents:\n",
    "            if agent not in obs.keys():\n",
    "                continue\n",
    "            obs1, obs2 = get_agent_obs(obs, agent)\n",
    "            Buffer.add(obs1, obs2, actions[agent], reward[agent], done[agent], log_prob=log_probs[agent], value=values[agent])\n",
    "        done = [done[agent] for agent in agents if agent in done.keys()]\n",
    "        tot_reward = [reward[agent] for agent in agents if agent in reward.keys()]\n",
    "    obs_keys = list(obs.keys())\n",
    "    _, _, last_values = brain.get_action(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2])\n",
    "    Buffer.add_final_state(obs[obs_keys[-1]][1], obs[obs_keys[-1]][2], last_values)\n",
    "    mean_reward = np.mean(tot_reward)\n",
    "    \n",
    "    steps += t\n",
    "\n",
    "    brain.train(steps, Buffer) \n",
    "    \n",
    "    Buffer.compute_advantages_and_returns()\n",
    "    brain.optimizer = brain.improv_lr(brain.optimizer, params['ppo_distilled'].lr,steps, params['ppo_distilled'].n_steps)\n",
    "    brain.optimizer_distill = brain.improv_lr(brain.optimizer_distill, params['ppo_distilled'].lr,steps, params['ppo_distilled'].n_steps)\n",
    "    wandb.log({\"Mean Reward\": mean_reward, \"Steps\": steps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(brain.net.state_dict(), \"PPO_distilled_checkpoint.pth\")\n",
    "# print(\"Checkpoint saved successfully.\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# brain.net.to(device)\n",
    "\n",
    "# # Ensure the model is in evaluation mode\n",
    "# brain.net.eval()\n",
    "\n",
    "# # Create dummy input matching the expected input format of the model\n",
    "# dummy_input_1 = torch.randn(1, *env.observation_space(agents[0])[1].shape).to(device)\n",
    "# dummy_input_2 = torch.randn(1, *env.observation_space(agents[0])[2].shape).to(device)\n",
    "\n",
    "# # Export the model to ONNX format\n",
    "# torch.onnx.export(\n",
    "#     brain.net,\n",
    "#     (dummy_input_1, dummy_input_2),\n",
    "#     \"PPO_distilled.onnx\",\n",
    "#     export_params=True,\n",
    "#     opset_version=10,\n",
    "#     do_constant_folding=True,\n",
    "#     input_names=[\"observation1\", \"observation2\"],\n",
    "#     output_names=[\"action\"],\n",
    "# )\n",
    "# print(\"Model exported to ONNX format successfully.\")\n",
    "\n",
    "# # Dispose of the dummy input tensors\n",
    "# del dummy_input_1\n",
    "# del dummy_input_2\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
