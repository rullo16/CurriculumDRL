behaviors:
  Drone:
    ##########################################################################
    # 1 ▸ algorithm
    trainer_type: ppo

    ##########################################################################
    # 2 ▸ PPO hyper-parameters
    hyperparameters:
      batch_size:        1024            #  ≈4×time_horizon → stable SGD
      buffer_size:       163840          #  40×batch_size
      learning_rate:     2.0e-4          #  lower because reward magnitudes ↓
      learning_rate_schedule: linear

      beta:              2.5e-4          #  entropy bonus
      beta_schedule:     constant        #  can decay later if explo → exploit
      epsilon:           0.15            #  clip range
      epsilon_schedule:  constant

      lambd:             0.97            #  GAE λ
      num_epoch:         4               #  M-size ×4 passes

    ##########################################################################
    # 3 ▸ network
    network_settings:
      normalize:   true
      hidden_units: 256
      num_layers: 2
      memory:                        # 1-layer LSTM smooths inertial control
        memory_size:     256
        sequence_length: 32
      vis_encode_type: resnet        #  keep conv off (no CameraSensor)

    ##########################################################################
    # 4 ▸ reward signals
    reward_signals:
      extrinsic:
        gamma:    0.99
        strength: 1.0
      rnd:                           #  exploration early on, then fades out
        gamma:    0.99
        strength: 0.02
        learning_rate: 1.0e-4
        network_settings:
          hidden_units: 256
          num_layers: 2

    ##########################################################################
    # 5 ▸ training schedule
    max_steps:           3.0e6        #  ~7 h on one RTX-30
    time_horizon:        256
    summary_freq:        10000
    checkpoint_interval: 500000
    keep_checkpoints:    5

    ##########################################################################
    # # 6 ▸ optional automatic RND decay (needs ML-Agents ≥ 2.4)
    # schedule:
    #   behavior_name: Drone
    #   steps: 1000000             # at 1 M steps…
    #   update: reward_signals.rnd.strength=0.0
