{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc30a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model path is valid: C:\\Users\\Fede\\Desktop\\MasterThesis\\PPO_Unity\\results\\PPO\\Drone\\Drone-4499957.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import onnxruntime as ort\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "import collections\n",
    "import os\n",
    "\n",
    "# ======================================================================================\n",
    "# --- ‚öôÔ∏è 1. CONFIGURATION: PLEASE EDIT THESE VALUES ---\n",
    "# ======================================================================================\n",
    "\n",
    "# --- Environment and Model Paths ---\n",
    "ENV_PATH = \"../Env/DroneFlightv1.exe\" # or \"builds/YourEnv.x86_64\"\n",
    "\n",
    "# --- PASTE YOUR ABSOLUTE PATH HERE using r\"...\" ---\n",
    "# This tells Python to treat backslashes as normal characters, fixing the SyntaxError.\n",
    "MODEL_PATH = r\"C:\\Users\\Fede\\Desktop\\MasterThesis\\PPO_Unity\\results\\PPO\\Drone\\Drone-4499957.onnx\"\n",
    "\n",
    "# --- Data Collection and Training Settings ---\n",
    "STEPS_TO_COLLECT = 5000\n",
    "REPLAY_BUFFER_CAPACITY = 10000\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "TRAINING_STEPS = 2000\n",
    "\n",
    "# --- ONNX Model Input/Output Names (pre-filled from your inspection output) ---\n",
    "OBS_INPUT_NAMES = [\"obs_0\", \"obs_1\", \"obs_2\", \"obs_3\"]\n",
    "ACTION_OUTPUT_NAME = \"deterministic_continuous_actions\"\n",
    "RECURRENT_IN_NAME = \"recurrent_in\"\n",
    "RECURRENT_OUT_NAME = \"recurrent_out\"\n",
    "\n",
    "# ======================================================================================\n",
    "# --- Path Validation ---\n",
    "# ======================================================================================\n",
    "\n",
    "# Check if the model file exists at the absolute path\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Model file not found at the specified path!\")\n",
    "    print(f\"Path given: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model path is valid: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02795b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting model: C:\\Users\\Fede\\Desktop\\MasterThesis\\PPO_Unity\\results\\PPO\\Drone\\Drone-4499957.onnx\n",
      "\n",
      "--- Model Inputs ---\n",
      "Input 0: Name: obs_0, Shape: ['batch', 3, 84, 84], Type: tensor(float)\n",
      "Input 1: Name: obs_1, Shape: ['batch', 68], Type: tensor(float)\n",
      "Input 2: Name: obs_2, Shape: ['batch', 4, 84, 84], Type: tensor(float)\n",
      "Input 3: Name: obs_3, Shape: ['batch', 64], Type: tensor(float)\n",
      "Input 4: Name: recurrent_in, Shape: ['batch', 1, 256], Type: tensor(float)\n",
      "\n",
      "--- Model Outputs ---\n",
      "Output 0: Name: version_number, Shape: [1], Type: tensor(float)\n",
      "Output 1: Name: memory_size, Shape: [1], Type: tensor(float)\n",
      "Output 2: Name: continuous_actions, Shape: ['batch', 4], Type: tensor(float)\n",
      "Output 3: Name: continuous_action_output_shape, Shape: [1], Type: tensor(float)\n",
      "Output 4: Name: deterministic_continuous_actions, Shape: ['Divdeterministic_continuous_actions_dim_0', 4], Type: tensor(float)\n",
      "Output 5: Name: recurrent_out, Shape: ['Transposerecurrent_out_dim_0', 1, 256], Type: tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# --- üîé 2. INSPECT THE ONNX MODEL ---\n",
    "# ======================================================================================\n",
    "\n",
    "# This cell is for verification. You've already provided the output, so the names in Cell 1 are correct.\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        print(f\"Inspecting model: {MODEL_PATH}\\n\")\n",
    "        session = ort.InferenceSession(MODEL_PATH)\n",
    "\n",
    "        print(\"--- Model Inputs ---\")\n",
    "        for i, inp in enumerate(session.get_inputs()):\n",
    "            print(f\"Input {i}: Name: {inp.name}, Shape: {inp.shape}, Type: {inp.type}\")\n",
    "\n",
    "        print(\"\\n--- Model Outputs ---\")\n",
    "        for i, out in enumerate(session.get_outputs()):\n",
    "            print(f\"Output {i}: Name: {out.name}, Shape: {out.shape}, Type: {out.type}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading or inspecting the ONNX model: {e}\")\n",
    "else:\n",
    "    print(\"Skipping inspection because model path is not valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e551ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# --- üì¶ 3. REPLAY BUFFER AND DATA GATHERING LOGIC (REVISED FOR ACTION HANDLING) ---\n",
    "# ======================================================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple FIFO experience replay buffer.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
    "        flat_states = np.array([np.concatenate([obs.flatten() for obs in s]) for s in states])\n",
    "        flat_next_states = np.array([np.concatenate([obs.flatten() for obs in ns]) for ns in next_states])\n",
    "        return flat_states, np.array(actions), np.array(rewards, dtype=np.float32), flat_next_states, np.array(dones, dtype=np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def gather_experience_recurrent(env, onnx_session, behavior_name, num_steps, spec):\n",
    "    \"\"\"Gathers experience from a recurrent, multi-input model.\"\"\"\n",
    "    print(\"\\n--- Starting Experience Gathering (Recurrent Mode) ---\")\n",
    "    replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
    "    \n",
    "    obs_names = OBS_INPUT_NAMES\n",
    "    recurrent_in = RECURRENT_IN_NAME\n",
    "    recurrent_out = RECURRENT_OUT_NAME\n",
    "    action_out_name = ACTION_OUTPUT_NAME\n",
    "    \n",
    "    agent_memory = collections.defaultdict(lambda: np.zeros((1, 256), dtype=np.float32))\n",
    "    agent_transition_data = {}\n",
    "\n",
    "    collected_steps = 0\n",
    "    while collected_steps < num_steps:\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        \n",
    "        for agent_id in terminal_steps:\n",
    "            if agent_id in agent_transition_data:\n",
    "                prev_obs, prev_action = agent_transition_data.pop(agent_id)\n",
    "                replay_buffer.add(prev_obs, prev_action, terminal_steps[agent_id].reward, terminal_steps[agent_id].obs, True)\n",
    "                collected_steps += 1\n",
    "            agent_memory.pop(agent_id, None)\n",
    "\n",
    "        if len(decision_steps) > 0:\n",
    "            input_feed = {}\n",
    "            for i, name in enumerate(obs_names):\n",
    "                input_feed[name] = decision_steps.obs[i]\n",
    "            \n",
    "            agent_ids = decision_steps.agent_id\n",
    "            \n",
    "            memory_array_2d = np.vstack([agent_memory[id] for id in agent_ids])\n",
    "            memory_array_3d = memory_array_2d[:, np.newaxis, :]\n",
    "            input_feed[recurrent_in] = memory_array_3d\n",
    "            \n",
    "            action_output, recurrent_output = onnx_session.run([action_out_name, recurrent_out], input_feed)\n",
    "            \n",
    "            for i, agent_id in enumerate(agent_ids):\n",
    "                agent_memory[agent_id] = recurrent_output[i, :, :]\n",
    "\n",
    "            for agent_id in decision_steps:\n",
    "                if agent_id in agent_transition_data:\n",
    "                    prev_obs, prev_action = agent_transition_data.pop(agent_id)\n",
    "                    replay_buffer.add(prev_obs, prev_action, decision_steps[agent_id].reward, decision_steps[agent_id].obs, False)\n",
    "                    collected_steps += 1\n",
    "                    if collected_steps % 250 == 0:\n",
    "                        print(f\"Collected {collected_steps}/{num_steps} steps... Buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "            for i, agent_id in enumerate(agent_ids):\n",
    "                agent_transition_data[agent_id] = ([obs[i] for obs in decision_steps.obs], action_output[i])\n",
    "\n",
    "            action_dict = {agent_id: action_output[i] for i, agent_id in enumerate(agent_ids)}\n",
    "            \n",
    "            # ======================= THIS IS THE FIX =========================\n",
    "            # Pass the raw numpy array of actions directly. \n",
    "            # The set_actions method is smart enough to handle this.\n",
    "            # env.set_actions(behavior_name, action_output)\n",
    "            # =================================================================\n",
    "\n",
    "        env.step(action_dict)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Finished gathering experience. Replay buffer has {len(replay_buffer)} entries.\")\n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d37b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# --- üß† 4. CUSTOM PYTORCH MODEL AND TRAINING LOOP (REVISED) ---\n",
    "# ======================================================================================\n",
    "\n",
    "class CustomActor(nn.Module):\n",
    "    \"\"\"A simple MLP policy network for behavioral cloning.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CustomActor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), # Increased size for complex state\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "def train_custom_model(replay_buffer, state_dim, action_dim):\n",
    "    \"\"\"Initializes and trains the custom model using behavioral cloning.\"\"\"\n",
    "    print(\"\\n--- Starting Custom Model Training ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    custom_model = CustomActor(state_dim, action_dim).to(device)\n",
    "    optimizer = optim.Adam(custom_model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for step in range(TRAINING_STEPS):\n",
    "        # The replay buffer now returns flattened states automatically\n",
    "        states, actions, _, _, _ = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        states_t = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "        expert_actions_t = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "\n",
    "        predicted_actions_t = custom_model(states_t)\n",
    "        loss = loss_fn(predicted_actions_t, expert_actions_t)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"Training Step: {step + 1}/{TRAINING_STEPS} | MSE Loss: {loss.item():.6f}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Custom model training complete.\")\n",
    "    return custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2baf76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experience Gathering (Recurrent Mode) ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "UnityEnvironment.step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m onnx_session \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(MODEL_PATH, providers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 2. Gather expert experience\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m experience_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mgather_experience_recurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbehavior_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_TO_COLLECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 3. Train the custom model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 82\u001b[0m, in \u001b[0;36mgather_experience_recurrent\u001b[1;34m(env, onnx_session, behavior_name, num_steps, spec)\u001b[0m\n\u001b[0;32m     74\u001b[0m         action_dict \u001b[38;5;241m=\u001b[39m {agent_id: action_output[i] \u001b[38;5;28;01mfor\u001b[39;00m i, agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_ids)}\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;66;03m# ======================= THIS IS THE FIX =========================\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# Pass the raw numpy array of actions directly. \u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# The set_actions method is smart enough to handle this.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# env.set_actions(behavior_name, action_output)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;66;03m# =================================================================\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Finished gathering experience. Replay buffer has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(replay_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m replay_buffer\n",
      "File \u001b[1;32mc:\\Users\\Fede\\anaconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: UnityEnvironment.step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# --- ‚ñ∂Ô∏è 5. EXECUTE THE WORKFLOW (REVISED) ---\n",
    "# ======================================================================================\n",
    "\n",
    "# 1. Launch Environment and Load ONNX Model\n",
    "if MODEL_PATH:\n",
    "    engine_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=ENV_PATH, side_channels=[engine_channel], worker_id=np.random.randint(0, 100))\n",
    "    engine_channel.set_configuration_parameters(time_scale=20.0)\n",
    "    env.reset()\n",
    "    \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    \n",
    "    onnx_session = ort.InferenceSession(MODEL_PATH, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    # 2. Gather expert experience\n",
    "    experience_buffer = gather_experience_recurrent(env, onnx_session, behavior_name, STEPS_TO_COLLECT, spec)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    # 3. Train the custom model\n",
    "    if experience_buffer and len(experience_buffer) > BATCH_SIZE:\n",
    "        # Get state and action dimensions by sampling the buffer\n",
    "        sample_flat_state, sample_action, _, _, _ = experience_buffer.sample(1)\n",
    "        state_dim = sample_flat_state.shape[1]\n",
    "        action_dim = sample_action.shape[1]\n",
    "        \n",
    "        print(f\"\\nDetected Flattened State Dimension: {state_dim}\")\n",
    "        print(f\"Detected Action Dimension: {action_dim}\")\n",
    "\n",
    "        trained_model = train_custom_model(experience_buffer, state_dim, action_dim)\n",
    "        \n",
    "        # Save the trained student model\n",
    "        torch.save(trained_model.state_dict(), \"my_custom_drone_model.pth\")\n",
    "        print(\"\\nCustom bootstrapped model saved to 'my_custom_drone_model.pth'\")\n",
    "    else:\n",
    "        print(\"\\nSkipping training due to an error or insufficient data.\")\n",
    "else:\n",
    "    print(\"Execution skipped because ONNX model path was not found. Please check your config in Cell 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687c670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
